{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Code Search using Transformers and BERT - Part II\n",
    "Author - Shashank Ramesh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part-II uses docstrings from the data collected and processed in part-I and converts them to vectors using ALBERT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting Docstrings to vectors\n",
    "This notebook contains steps to fine-tune ALBERT model to generate docstring vectors"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7s5TYp3q-oMx"
   },
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import csv\n",
    "import pandas as pd\n",
    "import transformers\n",
    "from transformers import  AlbertConfig,TFAlbertModel\n",
    "from transformers import AlbertTokenizer\n",
    "from transformers import  AlbertConfig,TFAlbertModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating training data to fine-tune ALBERT\n",
    "For incremental training on a pre-trained ALBERT model we need to process our docstrings for the model to train on. We need to generate masks and create sentence pairs for the model to train on using the n-gram Masked Language Modelling and Sentence Order Prediction tasks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "RNyTKe5Q-60v",
    "outputId": "0fe977b3-bb8e-476d-843f-fdc94367c499",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/albert/tokenization.py:240: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
      "\n",
      "W0522 21:01:59.654605 140550058289024 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/albert/tokenization.py:240: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
      "\n",
      "INFO:tensorflow:loading sentence piece model\n",
      "I0522 21:01:59.654801 140550058289024 tokenization.py:240] loading sentence piece model\n",
      "INFO:tensorflow:*** Reading from input files ***\n",
      "I0522 21:01:59.730216 140550058289024 create_pretraining_data.py:631] *** Reading from input files ***\n",
      "INFO:tensorflow:  /content/drive/My Drive/BERT Training/docstrings.txt\n",
      "I0522 21:01:59.730445 140550058289024 create_pretraining_data.py:633]   /content/drive/My Drive/BERT Training/docstrings.txt\n",
      "INFO:tensorflow:number of instances: 1720445\n",
      "I0522 21:22:03.437951 140550058289024 create_pretraining_data.py:641] number of instances: 1720445\n",
      "INFO:tensorflow:*** Writing to output files ***\n",
      "I0522 21:22:03.438693 140550058289024 create_pretraining_data.py:644] *** Writing to output files ***\n",
      "INFO:tensorflow:  /content/drive/My Drive/BERT Training/tf_examples\n",
      "I0522 21:22:03.438786 140550058289024 create_pretraining_data.py:646]   /content/drive/My Drive/BERT Training/tf_examples\n",
      "INFO:tensorflow:*** Example ***\n",
      "I0522 21:22:03.442604 140550058289024 create_pretraining_data.py:188] *** Example ***\n",
      "INFO:tensorflow:tokens: [CLS] ▁to ▁a ▁file ▁determine [MASK] ▁it ▁means ▁the ▁device ▁is ▁re mov able ▁or [SEP] ▁g [MASK] ▁c cis s ▁c 0 d 0 p 0 [MASK] [MASK] [SEP]\n",
      "I0522 21:22:03.442790 140550058289024 create_pretraining_data.py:190] tokens: [CLS] ▁to ▁a ▁file ▁determine [MASK] ▁it ▁means ▁the ▁device ▁is ▁re mov able ▁or [SEP] ▁g [MASK] ▁c cis s ▁c 0 d 0 p 0 [MASK] [MASK] [SEP]\n",
      "INFO:tensorflow:input_ids: 2 20 21 3893 3746 4 32 1108 14 3646 25 302 15146 579 54 3 489 4 272 7654 18 272 387 43 387 306 387 4 4 3\n",
      "I0522 21:22:03.442959 140550058289024 create_pretraining_data.py:200] input_ids: 2 20 21 3893 3746 4 32 1108 14 3646 25 302 15146 579 54 3 489 4 272 7654 18 272 387 43 387 306 387 4 4 3\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "I0522 21:22:03.443063 140550058289024 create_pretraining_data.py:200] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "I0522 21:22:03.443153 140550058289024 create_pretraining_data.py:200] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:tensorflow:token_boundary: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 0 0 1 1 0 1 0 1 1 1 1\n",
      "I0522 21:22:03.443247 140550058289024 create_pretraining_data.py:200] token_boundary: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 0 0 1 1 0 1 0 1 1 1 1\n",
      "INFO:tensorflow:masked_lm_positions: 5 17 27 28 0 0\n",
      "I0522 21:22:03.443327 140550058289024 create_pretraining_data.py:200] masked_lm_positions: 5 17 27 28 0 0\n",
      "INFO:tensorflow:masked_lm_ids: 100 9664 14 25 0 0\n",
      "I0522 21:22:03.443400 140550058289024 create_pretraining_data.py:200] masked_lm_ids: 100 9664 14 25 0 0\n",
      "INFO:tensorflow:masked_lm_weights: 1.0 1.0 1.0 1.0 0.0 0.0\n",
      "I0522 21:22:03.443550 140550058289024 create_pretraining_data.py:200] masked_lm_weights: 1.0 1.0 1.0 1.0 0.0 0.0\n",
      "INFO:tensorflow:next_sentence_labels: 0\n",
      "I0522 21:22:03.443640 140550058289024 create_pretraining_data.py:200] next_sentence_labels: 0\n",
      "INFO:tensorflow:*** Example ***\n",
      "I0522 21:22:03.443980 140550058289024 create_pretraining_data.py:188] *** Example ***\n",
      "INFO:tensorflow:tokens: [CLS] ▁member [MASK] [MASK] ▁from ▁the ▁node ▁id ▁for ▁a ▁etc d ▁member ▁of ▁the [SEP] ▁id ▁of ▁the ▁running ▁container ▁named [MASK] ▁running ▁inside ▁of ▁the ▁specified ▁name [SEP]\n",
      "I0522 21:22:03.444104 140550058289024 create_pretraining_data.py:190] tokens: [CLS] ▁member [MASK] [MASK] ▁from ▁the ▁node ▁id ▁for ▁a ▁etc d ▁member ▁of ▁the [SEP] ▁id ▁of ▁the ▁running ▁container ▁named [MASK] ▁running ▁inside ▁of ▁the ▁specified ▁name [SEP]\n",
      "INFO:tensorflow:input_ids: 2 322 4 4 37 14 15421 4924 26 21 2722 43 322 16 14 3 4924 16 14 946 12147 377 4 946 572 16 14 9931 204 3\n",
      "I0522 21:22:03.444219 140550058289024 create_pretraining_data.py:200] input_ids: 2 322 4 4 37 14 15421 4924 26 21 2722 43 322 16 14 3 4924 16 14 946 12147 377 4 946 572 16 14 9931 204 3\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "I0522 21:22:03.444329 140550058289024 create_pretraining_data.py:200] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "I0522 21:22:03.444414 140550058289024 create_pretraining_data.py:200] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:tensorflow:token_boundary: 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "I0522 21:22:03.444495 140550058289024 create_pretraining_data.py:200] token_boundary: 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:tensorflow:masked_lm_positions: 2 3 16 22 0 0\n",
      "I0522 21:22:03.444566 140550058289024 create_pretraining_data.py:200] masked_lm_positions: 2 3 16 22 0 0\n",
      "INFO:tensorflow:masked_lm_ids: 4924 421 4924 204 0 0\n",
      "I0522 21:22:03.444635 140550058289024 create_pretraining_data.py:200] masked_lm_ids: 4924 421 4924 204 0 0\n",
      "INFO:tensorflow:masked_lm_weights: 1.0 1.0 1.0 1.0 0.0 0.0\n",
      "I0522 21:22:03.444707 140550058289024 create_pretraining_data.py:200] masked_lm_weights: 1.0 1.0 1.0 1.0 0.0 0.0\n",
      "INFO:tensorflow:next_sentence_labels: 0\n",
      "I0522 21:22:03.444800 140550058289024 create_pretraining_data.py:200] next_sentence_labels: 0\n",
      "INFO:tensorflow:*** Example ***\n",
      "I0522 21:22:03.445098 140550058289024 create_pretraining_data.py:188] *** Example ***\n",
      "INFO:tensorflow:tokens: [CLS] ▁out ▁a ▁table ▁of ▁data [MASK] [MASK] ▁images ▁each ▁row [MASK] ▁have ▁the ▁same ▁number [SEP] ▁a ▁j son ▁serial iz able ▁ dict ▁representation ▁of ▁an ▁object [SEP]\n",
      "I0522 21:22:03.445211 140550058289024 create_pretraining_data.py:190] tokens: [CLS] ▁out ▁a ▁table ▁of ▁data [MASK] [MASK] ▁images ▁each ▁row [MASK] ▁have ▁the ▁same ▁number [SEP] ▁a ▁j son ▁serial iz able ▁ dict ▁representation ▁of ▁an ▁object [SEP]\n",
      "INFO:tensorflow:input_ids: 2 70 21 859 16 1054 4 4 3502 206 3131 4 57 14 205 234 3 21 487 528 5956 3186 579 13 16315 5442 16 40 3095 3\n",
      "I0522 21:22:03.445330 140550058289024 create_pretraining_data.py:200] input_ids: 2 70 21 859 16 1054 4 4 3502 206 3131 4 57 14 205 234 3 21 487 528 5956 3186 579 13 16315 5442 16 40 3095 3\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "I0522 21:22:03.445416 140550058289024 create_pretraining_data.py:200] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "I0522 21:22:03.445500 140550058289024 create_pretraining_data.py:200] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:tensorflow:token_boundary: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1\n",
      "I0522 21:22:03.445579 140550058289024 create_pretraining_data.py:200] token_boundary: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1\n",
      "INFO:tensorflow:masked_lm_positions: 6 7 8 11 0 0\n",
      "I0522 21:22:03.445648 140550058289024 create_pretraining_data.py:200] masked_lm_positions: 6 7 8 11 0 0\n",
      "INFO:tensorflow:masked_lm_ids: 24013 26 12448 491 0 0\n",
      "I0522 21:22:03.445722 140550058289024 create_pretraining_data.py:200] masked_lm_ids: 24013 26 12448 491 0 0\n",
      "INFO:tensorflow:masked_lm_weights: 1.0 1.0 1.0 1.0 0.0 0.0\n",
      "I0522 21:22:03.445795 140550058289024 create_pretraining_data.py:200] masked_lm_weights: 1.0 1.0 1.0 1.0 0.0 0.0\n",
      "INFO:tensorflow:next_sentence_labels: 0\n",
      "I0522 21:22:03.445861 140550058289024 create_pretraining_data.py:200] next_sentence_labels: 0\n",
      "INFO:tensorflow:*** Example ***\n",
      "I0522 21:22:03.446135 140550058289024 create_pretraining_data.py:188] *** Example ***\n",
      "INFO:tensorflow:tokens: [CLS] ▁within ▁match ▁radius ▁to ▁ ra 1 ▁dec 1 ▁ ra [MASK] [MASK] 1 [SEP] ▁a ▁parallel ▁worker ▁for ▁the ▁function ▁below ▁this ▁tumbled ▁a [MASK] ▁worker ▁for [SEP]\n",
      "I0522 21:22:03.446255 140550058289024 create_pretraining_data.py:190] tokens: [CLS] ▁within ▁match ▁radius ▁to ▁ ra 1 ▁dec 1 ▁ ra [MASK] [MASK] 1 [SEP] ▁a ▁parallel ▁worker ▁for ▁the ▁function ▁below ▁this ▁tumbled ▁a [MASK] ▁worker ▁for [SEP]\n",
      "INFO:tensorflow:input_ids: 2 363 730 13288 20 13 525 165 6661 165 13 525 4 4 165 3 21 3821 7444 26 14 1990 1021 48 21105 21 4 7444 26 3\n",
      "I0522 21:22:03.446357 140550058289024 create_pretraining_data.py:200] input_ids: 2 363 730 13288 20 13 525 165 6661 165 13 525 4 4 165 3 21 3821 7444 26 14 1990 1021 48 21105 21 4 7444 26 3\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "I0522 21:22:03.446440 140550058289024 create_pretraining_data.py:200] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "I0522 21:22:03.446546 140550058289024 create_pretraining_data.py:200] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:tensorflow:token_boundary: 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "I0522 21:22:03.446631 140550058289024 create_pretraining_data.py:200] token_boundary: 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:tensorflow:masked_lm_positions: 12 13 24 26 0 0\n",
      "I0522 21:22:03.446705 140550058289024 create_pretraining_data.py:200] masked_lm_positions: 12 13 24 26 0 0\n",
      "INFO:tensorflow:masked_lm_ids: 165 6661 25 3821 0 0\n",
      "I0522 21:22:03.446785 140550058289024 create_pretraining_data.py:200] masked_lm_ids: 165 6661 25 3821 0 0\n",
      "INFO:tensorflow:masked_lm_weights: 1.0 1.0 1.0 1.0 0.0 0.0\n",
      "I0522 21:22:03.446862 140550058289024 create_pretraining_data.py:200] masked_lm_weights: 1.0 1.0 1.0 1.0 0.0 0.0\n",
      "INFO:tensorflow:next_sentence_labels: 1\n",
      "I0522 21:22:03.446932 140550058289024 create_pretraining_data.py:200] next_sentence_labels: 1\n",
      "INFO:tensorflow:*** Example ***\n",
      "I0522 21:22:03.447259 140550058289024 create_pretraining_data.py:188] *** Example ***\n",
      "INFO:tensorflow:tokens: [CLS] [MASK] ▁a ▁python ▁date time ▁object ▁and ▁returns ▁a ▁string ▁in ▁ y y y y mm [SEP] ▁calculate [MASK] ▁de clin ation ▁of ▁the ▁sun [MASK] ▁premiership [SEP]\n",
      "I0522 21:22:03.447374 140550058289024 create_pretraining_data.py:190] tokens: [CLS] [MASK] ▁a ▁python ▁date time ▁object ▁and ▁returns ▁a ▁string ▁in ▁ y y y y mm [SEP] ▁calculate [MASK] ▁de clin ation ▁of ▁the ▁sun [MASK] ▁premiership [SEP]\n",
      "INFO:tensorflow:input_ids: 2 4 21 20059 1231 891 3095 17 4815 21 3724 19 13 93 93 93 93 3363 3 18469 4 121 16799 857 16 14 939 4 10780 3\n",
      "I0522 21:22:03.447478 140550058289024 create_pretraining_data.py:200] input_ids: 2 4 21 20059 1231 891 3095 17 4815 21 3724 19 13 93 93 93 93 3363 3 18469 4 121 16799 857 16 14 939 4 10780 3\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "I0522 21:22:03.447592 140550058289024 create_pretraining_data.py:200] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1\n",
      "I0522 21:22:03.447707 140550058289024 create_pretraining_data.py:200] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:tensorflow:token_boundary: 1 1 1 1 1 0 1 1 1 1 1 1 1 0 0 0 0 0 1 1 1 1 0 0 1 1 1 1 1 1\n",
      "I0522 21:22:03.447804 140550058289024 create_pretraining_data.py:200] token_boundary: 1 1 1 1 1 0 1 1 1 1 1 1 1 0 0 0 0 0 1 1 1 1 0 0 1 1 1 1 1 1\n",
      "INFO:tensorflow:masked_lm_positions: 1 20 27 28 0 0\n",
      "I0522 21:22:03.447891 140550058289024 create_pretraining_data.py:200] masked_lm_positions: 1 20 27 28 0 0\n",
      "INFO:tensorflow:masked_lm_ids: 1384 14 19 4442 0 0\n",
      "I0522 21:22:03.447979 140550058289024 create_pretraining_data.py:200] masked_lm_ids: 1384 14 19 4442 0 0\n",
      "INFO:tensorflow:masked_lm_weights: 1.0 1.0 1.0 1.0 0.0 0.0\n",
      "I0522 21:22:03.448079 140550058289024 create_pretraining_data.py:200] masked_lm_weights: 1.0 1.0 1.0 1.0 0.0 0.0\n",
      "INFO:tensorflow:next_sentence_labels: 1\n",
      "I0522 21:22:03.448155 140550058289024 create_pretraining_data.py:200] next_sentence_labels: 1\n",
      "INFO:tensorflow:*** Example ***\n",
      "I0522 21:22:03.448448 140550058289024 create_pretraining_data.py:188] *** Example ***\n",
      "INFO:tensorflow:tokens: [CLS] [MASK] [MASK] ▁off ▁method [SEP] ▁device ▁haines ▁setup ▁the ▁de non ▁platform ▁flag ▁media [MASK] ▁features ▁that ▁are ▁supported ▁return ▁all ▁module ▁available ▁on ▁the ▁a pi ▁as [SEP]\n",
      "I0522 21:22:03.448578 140550058289024 create_pretraining_data.py:190] tokens: [CLS] [MASK] [MASK] ▁off ▁method [SEP] ▁device ▁haines ▁setup ▁the ▁de non ▁platform ▁flag ▁media [MASK] ▁features ▁that ▁are ▁supported ▁return ▁all ▁module ▁available ▁on ▁the ▁a pi ▁as [SEP]\n",
      "INFO:tensorflow:input_ids: 2 4 4 168 2109 3 3646 28872 18161 14 121 3951 2452 3157 941 4 967 30 50 1827 788 65 12613 904 27 14 21 2159 28 3\n",
      "I0522 21:22:03.448686 140550058289024 create_pretraining_data.py:200] input_ids: 2 4 4 168 2109 3 3646 28872 18161 14 121 3951 2452 3157 941 4 967 30 50 1827 788 65 12613 904 27 14 21 2159 28 3\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "I0522 21:22:03.448775 140550058289024 create_pretraining_data.py:200] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "I0522 21:22:03.448857 140550058289024 create_pretraining_data.py:200] segment_ids: 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:tensorflow:token_boundary: 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1\n",
      "I0522 21:22:03.448935 140550058289024 create_pretraining_data.py:200] token_boundary: 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1\n",
      "INFO:tensorflow:masked_lm_positions: 1 2 7 15 0 0\n",
      "I0522 21:22:03.449002 140550058289024 create_pretraining_data.py:200] masked_lm_positions: 1 2 7 15 0 0\n",
      "INFO:tensorflow:masked_lm_ids: 1289 805 146 517 0 0\n",
      "I0522 21:22:03.449070 140550058289024 create_pretraining_data.py:200] masked_lm_ids: 1289 805 146 517 0 0\n",
      "INFO:tensorflow:masked_lm_weights: 1.0 1.0 1.0 1.0 0.0 0.0\n",
      "I0522 21:22:03.449139 140550058289024 create_pretraining_data.py:200] masked_lm_weights: 1.0 1.0 1.0 1.0 0.0 0.0\n",
      "INFO:tensorflow:next_sentence_labels: 0\n",
      "I0522 21:22:03.449203 140550058289024 create_pretraining_data.py:200] next_sentence_labels: 0\n",
      "INFO:tensorflow:*** Example ***\n",
      "I0522 21:22:03.449492 140550058289024 create_pretraining_data.py:188] *** Example ***\n",
      "INFO:tensorflow:tokens: [CLS] ▁the ▁connection ▁is ▁interrupted ▁con flu ent ▁ka f ka ▁configuration ▁flush ▁values ▁with [MASK] ▁create [SEP] ▁if ▁the ▁buffer ▁has ▁time d [MASK] [MASK] [MASK] ▁the ▁writer [SEP]\n",
      "I0522 21:22:03.449602 140550058289024 create_pretraining_data.py:190] tokens: [CLS] ▁the ▁connection ▁is ▁interrupted ▁con flu ent ▁ka f ka ▁configuration ▁flush ▁values ▁with [MASK] ▁create [SEP] ▁if ▁the ▁buffer ▁has ▁time d [MASK] [MASK] [MASK] ▁the ▁writer [SEP]\n",
      "INFO:tensorflow:input_ids: 2 14 2760 25 6354 1065 12848 2291 1332 410 657 8091 15017 4070 29 4 1600 3 100 14 17497 63 85 43 4 4 4 14 1462 3\n",
      "I0522 21:22:03.449699 140550058289024 create_pretraining_data.py:200] input_ids: 2 14 2760 25 6354 1065 12848 2291 1332 410 657 8091 15017 4070 29 4 1600 3 100 14 17497 63 85 43 4 4 4 14 1462 3\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "I0522 21:22:03.449788 140550058289024 create_pretraining_data.py:200] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "I0522 21:22:03.449872 140550058289024 create_pretraining_data.py:200] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:tensorflow:token_boundary: 1 1 1 1 1 1 0 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1\n",
      "I0522 21:22:03.449953 140550058289024 create_pretraining_data.py:200] token_boundary: 1 1 1 1 1 1 0 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1\n",
      "INFO:tensorflow:masked_lm_positions: 15 24 25 26 0 0\n",
      "I0522 21:22:03.450021 140550058289024 create_pretraining_data.py:200] masked_lm_positions: 15 24 25 26 0 0\n",
      "INFO:tensorflow:masked_lm_ids: 1462 70 15017 20 0 0\n",
      "I0522 21:22:03.450091 140550058289024 create_pretraining_data.py:200] masked_lm_ids: 1462 70 15017 20 0 0\n",
      "INFO:tensorflow:masked_lm_weights: 1.0 1.0 1.0 1.0 0.0 0.0\n",
      "I0522 21:22:03.450160 140550058289024 create_pretraining_data.py:200] masked_lm_weights: 1.0 1.0 1.0 1.0 0.0 0.0\n",
      "INFO:tensorflow:next_sentence_labels: 0\n",
      "I0522 21:22:03.450226 140550058289024 create_pretraining_data.py:200] next_sentence_labels: 0\n",
      "INFO:tensorflow:*** Example ***\n",
      "I0522 21:22:03.450511 140550058289024 create_pretraining_data.py:188] *** Example ***\n",
      "INFO:tensorflow:tokens: [CLS] ▁get ▁name ▁of ▁the ▁reduced ▁fast a ▁file ▁k ▁ mer ▁propagation [SEP] ▁make [MASK] [MASK] [MASK] ▁re cursive ly ▁no ▁error ▁if ▁existing ▁make [MASK] ▁director ies [SEP]\n",
      "I0522 21:22:03.450619 140550058289024 create_pretraining_data.py:190] tokens: [CLS] ▁get ▁name ▁of ▁the ▁reduced ▁fast a ▁file ▁k ▁ mer ▁propagation [SEP] ▁make [MASK] [MASK] [MASK] ▁re cursive ly ▁no ▁error ▁if ▁existing ▁make [MASK] ▁director ies [SEP]\n",
      "INFO:tensorflow:input_ids: 2 164 204 16 14 2736 1512 58 3893 680 13 1263 23664 3 233 4 4 4 302 24244 102 90 7019 100 3149 233 4 559 1596 3\n",
      "I0522 21:22:03.450720 140550058289024 create_pretraining_data.py:200] input_ids: 2 164 204 16 14 2736 1512 58 3893 680 13 1263 23664 3 233 4 4 4 302 24244 102 90 7019 100 3149 233 4 559 1596 3\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "I0522 21:22:03.450803 140550058289024 create_pretraining_data.py:200] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "I0522 21:22:03.450886 140550058289024 create_pretraining_data.py:200] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:tensorflow:token_boundary: 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0 0 1 0 0 1 1 1 1 1 1 1 0 1\n",
      "I0522 21:22:03.450968 140550058289024 create_pretraining_data.py:200] token_boundary: 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0 0 1 0 0 1 1 1 1 1 1 1 0 1\n",
      "INFO:tensorflow:masked_lm_positions: 15 16 17 26 0 0\n",
      "I0522 21:22:03.451038 140550058289024 create_pretraining_data.py:200] masked_lm_positions: 15 16 17 26 0 0\n",
      "INFO:tensorflow:masked_lm_ids: 13 9035 18 4766 0 0\n",
      "I0522 21:22:03.451107 140550058289024 create_pretraining_data.py:200] masked_lm_ids: 13 9035 18 4766 0 0\n",
      "INFO:tensorflow:masked_lm_weights: 1.0 1.0 1.0 1.0 0.0 0.0\n",
      "I0522 21:22:03.516261 140550058289024 create_pretraining_data.py:200] masked_lm_weights: 1.0 1.0 1.0 1.0 0.0 0.0\n",
      "INFO:tensorflow:next_sentence_labels: 0\n",
      "I0522 21:22:03.516393 140550058289024 create_pretraining_data.py:200] next_sentence_labels: 0\n",
      "INFO:tensorflow:*** Example ***\n",
      "I0522 21:22:03.516838 140550058289024 create_pretraining_data.py:188] *** Example ***\n",
      "INFO:tensorflow:tokens: [CLS] ▁number ▁of ▁rows ▁for ▁a ▁layout [MASK] [MASK] ▁ s ▁using ▁fixed ▁grid ▁format [SEP] ▁to ▁sort ▁modules ▁by [MASK] ▁keys ▁get ▁all ▁available ▁chart [MASK] ▁names ▁from [SEP]\n",
      "I0522 21:22:03.516993 140550058289024 create_pretraining_data.py:190] tokens: [CLS] ▁number ▁of ▁rows ▁for ▁a ▁layout [MASK] [MASK] ▁ s ▁using ▁fixed ▁grid ▁format [SEP] ▁to ▁sort ▁modules ▁by [MASK] ▁keys ▁get ▁all ▁available ▁chart [MASK] ▁names ▁from [SEP]\n",
      "INFO:tensorflow:input_ids: 2 234 16 11295 26 21 9106 4 4 13 18 568 3535 7354 2595 3 20 2058 17113 34 4 5534 164 65 904 1795 4 1817 37 3\n",
      "I0522 21:22:03.517140 140550058289024 create_pretraining_data.py:200] input_ids: 2 234 16 11295 26 21 9106 4 4 13 18 568 3535 7354 2595 3 20 2058 17113 34 4 5534 164 65 904 1795 4 1817 37 3\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "I0522 21:22:03.517285 140550058289024 create_pretraining_data.py:200] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "I0522 21:22:03.517414 140550058289024 create_pretraining_data.py:200] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:tensorflow:token_boundary: 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "I0522 21:22:03.517548 140550058289024 create_pretraining_data.py:200] token_boundary: 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:tensorflow:masked_lm_positions: 7 8 20 26 0 0\n",
      "I0522 21:22:03.517673 140550058289024 create_pretraining_data.py:200] masked_lm_positions: 7 8 20 26 0 0\n",
      "INFO:tensorflow:masked_lm_ids: 100 32 389 1001 0 0\n",
      "I0522 21:22:03.517809 140550058289024 create_pretraining_data.py:200] masked_lm_ids: 100 32 389 1001 0 0\n",
      "INFO:tensorflow:masked_lm_weights: 1.0 1.0 1.0 1.0 0.0 0.0\n",
      "I0522 21:22:03.517966 140550058289024 create_pretraining_data.py:200] masked_lm_weights: 1.0 1.0 1.0 1.0 0.0 0.0\n",
      "INFO:tensorflow:next_sentence_labels: 1\n",
      "I0522 21:22:03.518072 140550058289024 create_pretraining_data.py:200] next_sentence_labels: 1\n",
      "INFO:tensorflow:*** Example ***\n",
      "I0522 21:22:03.518485 140550058289024 create_pretraining_data.py:188] *** Example ***\n",
      "INFO:tensorflow:tokens: [CLS] ▁this ▁method ▁inherit ▁this ▁function ▁in ▁order ▁to ▁add ▁checks ▁to ▁cash ▁and ▁bank [MASK] [SEP] [MASK] ▁next [MASK] [MASK] ▁for ▁payments ▁without ▁number ▁and ▁on ▁draft ▁state [SEP]\n",
      "I0522 21:22:03.518622 140550058289024 create_pretraining_data.py:190] tokens: [CLS] ▁this ▁method ▁inherit ▁this ▁function ▁in ▁order ▁to ▁add ▁checks ▁to ▁cash ▁and ▁bank [MASK] [SEP] [MASK] ▁next [MASK] [MASK] ▁for ▁payments ▁without ▁number ▁and ▁on ▁draft ▁state [SEP]\n",
      "INFO:tensorflow:input_ids: 2 48 2109 17569 48 1990 19 389 20 3547 16602 20 3392 17 965 4 3 4 328 4 4 26 11161 366 234 17 27 2472 146 3\n",
      "I0522 21:22:03.518741 140550058289024 create_pretraining_data.py:200] input_ids: 2 48 2109 17569 48 1990 19 389 20 3547 16602 20 3392 17 965 4 3 4 328 4 4 26 11161 366 234 17 27 2472 146 3\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "I0522 21:22:03.518844 140550058289024 create_pretraining_data.py:200] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "I0522 21:22:03.518929 140550058289024 create_pretraining_data.py:200] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:tensorflow:token_boundary: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "I0522 21:22:03.519009 140550058289024 create_pretraining_data.py:200] token_boundary: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:tensorflow:masked_lm_positions: 15 17 19 20 0 0\n",
      "I0522 21:22:03.519078 140550058289024 create_pretraining_data.py:200] masked_lm_positions: 15 17 19 20 0 0\n",
      "INFO:tensorflow:masked_lm_ids: 9942 298 234 104 0 0\n",
      "I0522 21:22:03.519147 140550058289024 create_pretraining_data.py:200] masked_lm_ids: 9942 298 234 104 0 0\n",
      "INFO:tensorflow:masked_lm_weights: 1.0 1.0 1.0 1.0 0.0 0.0\n",
      "I0522 21:22:03.519219 140550058289024 create_pretraining_data.py:200] masked_lm_weights: 1.0 1.0 1.0 1.0 0.0 0.0\n",
      "INFO:tensorflow:next_sentence_labels: 1\n",
      "I0522 21:22:03.519309 140550058289024 create_pretraining_data.py:200] next_sentence_labels: 1\n",
      "INFO:tensorflow:*** Example ***\n",
      "I0522 21:22:03.519583 140550058289024 create_pretraining_data.py:188] *** Example ***\n",
      "INFO:tensorflow:tokens: [CLS] ▁loads ▁4 ▁different ▁sci kit ▁learn ▁re gress or s [MASK] ▁default [SEP] ▁data ▁into ▁different ▁parts ▁for ▁training ▁machines ▁and ▁for ▁aggregation [MASK] ▁the ▁data [MASK] ▁different [SEP]\n",
      "I0522 21:22:03.519702 140550058289024 create_pretraining_data.py:190] tokens: [CLS] ▁loads ▁4 ▁different ▁sci kit ▁learn ▁re gress or s [MASK] ▁default [SEP] ▁data ▁into ▁different ▁parts ▁for ▁training ▁machines ▁and ▁for ▁aggregation [MASK] ▁the ▁data [MASK] ▁different [SEP]\n",
      "INFO:tensorflow:input_ids: 2 19069 268 421 9569 13703 2484 302 13026 248 18 4 12838 3 1054 77 421 1341 26 838 6035 17 26 27255 4 14 1054 4 421 3\n",
      "I0522 21:22:03.519808 140550058289024 create_pretraining_data.py:200] input_ids: 2 19069 268 421 9569 13703 2484 302 13026 248 18 4 12838 3 1054 77 421 1341 26 838 6035 17 26 27255 4 14 1054 4 421 3\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "I0522 21:22:03.519892 140550058289024 create_pretraining_data.py:200] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "I0522 21:22:03.519976 140550058289024 create_pretraining_data.py:200] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:tensorflow:token_boundary: 1 1 1 1 1 0 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "I0522 21:22:03.520076 140550058289024 create_pretraining_data.py:200] token_boundary: 1 1 1 1 1 0 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:tensorflow:masked_lm_positions: 11 22 24 27 0 0\n",
      "I0522 21:22:03.520148 140550058289024 create_pretraining_data.py:200] masked_lm_positions: 11 22 24 27 0 0\n",
      "INFO:tensorflow:masked_lm_ids: 34 26 2132 77 0 0\n",
      "I0522 21:22:03.520221 140550058289024 create_pretraining_data.py:200] masked_lm_ids: 34 26 2132 77 0 0\n",
      "INFO:tensorflow:masked_lm_weights: 1.0 1.0 1.0 1.0 0.0 0.0\n",
      "I0522 21:22:03.520322 140550058289024 create_pretraining_data.py:200] masked_lm_weights: 1.0 1.0 1.0 1.0 0.0 0.0\n",
      "INFO:tensorflow:next_sentence_labels: 1\n",
      "I0522 21:22:03.520393 140550058289024 create_pretraining_data.py:200] next_sentence_labels: 1\n",
      "INFO:tensorflow:*** Example ***\n",
      "I0522 21:22:03.520679 140550058289024 create_pretraining_data.py:188] *** Example ***\n",
      "INFO:tensorflow:tokens: [CLS] ▁of ▁two ▁lines ▁we ▁can ▁specify ▁either ▁a ▁revision ▁or [MASK] ▁branch ▁for ▁comparison ▁the [MASK] ▁rev [MASK] ▁be [SEP] ▁check ▁that ▁items ▁are ▁cleared ▁from ▁the ▁cache [SEP]\n",
      "I0522 21:22:03.520792 140550058289024 create_pretraining_data.py:190] tokens: [CLS] ▁of ▁two ▁lines ▁we ▁can ▁specify ▁either ▁a ▁revision ▁or [MASK] ▁branch ▁for ▁comparison ▁the [MASK] ▁rev [MASK] ▁be [SEP] ▁check ▁that ▁items ▁are ▁cleared ▁from ▁the ▁cache [SEP]\n",
      "INFO:tensorflow:input_ids: 2 16 81 1560 95 92 19077 694 21 11323 54 4 1686 26 6050 14 4 3867 4 44 3 2631 30 3755 50 4895 37 14 16522 3\n",
      "I0522 21:22:03.520889 140550058289024 create_pretraining_data.py:200] input_ids: 2 16 81 1560 95 92 19077 694 21 11323 54 4 1686 26 6050 14 4 3867 4 44 3 2631 30 3755 50 4895 37 14 16522 3\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "I0522 21:22:03.520972 140550058289024 create_pretraining_data.py:200] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n",
      "I0522 21:22:03.521057 140550058289024 create_pretraining_data.py:200] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n",
      "INFO:tensorflow:token_boundary: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "I0522 21:22:03.521136 140550058289024 create_pretraining_data.py:200] token_boundary: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:tensorflow:masked_lm_positions: 11 16 17 18 0 0\n",
      "I0522 21:22:03.521204 140550058289024 create_pretraining_data.py:200] masked_lm_positions: 11 16 17 18 0 0\n",
      "INFO:tensorflow:masked_lm_ids: 21 64 3867 378 0 0\n",
      "I0522 21:22:03.521291 140550058289024 create_pretraining_data.py:200] masked_lm_ids: 21 64 3867 378 0 0\n",
      "INFO:tensorflow:masked_lm_weights: 1.0 1.0 1.0 1.0 0.0 0.0\n",
      "I0522 21:22:03.521363 140550058289024 create_pretraining_data.py:200] masked_lm_weights: 1.0 1.0 1.0 1.0 0.0 0.0\n",
      "INFO:tensorflow:next_sentence_labels: 1\n",
      "I0522 21:22:03.521428 140550058289024 create_pretraining_data.py:200] next_sentence_labels: 1\n",
      "INFO:tensorflow:*** Example ***\n",
      "I0522 21:22:03.521694 140550058289024 create_pretraining_data.py:188] *** Example ***\n",
      "INFO:tensorflow:tokens: [CLS] ser ▁with ▁metric s ▁related ▁options ▁pre process ▁the ▁ arg s ▁to ▁determine ▁the [SEP] ▁call ▁the ▁function ▁on ▁each ▁of ▁the [MASK] hatta [MASK] legged ▁instances [SEP]\n",
      "I0522 21:22:03.521805 140550058289024 create_pretraining_data.py:190] tokens: [CLS] ser ▁with ▁metric s ▁related ▁options ▁pre process ▁the ▁ arg s ▁to ▁determine ▁the [SEP] ▁call ▁the ▁function ▁on ▁each ▁of ▁the [MASK] hatta [MASK] legged ▁instances [SEP]\n",
      "INFO:tensorflow:input_ids: 2 4104 29 11544 18 1597 6368 782 16835 14 13 10663 18 20 3746 14 3 645 14 1990 27 206 16 14 4 29130 4 17512 13946 3\n",
      "I0522 21:22:03.521902 140550058289024 create_pretraining_data.py:200] input_ids: 2 4104 29 11544 18 1597 6368 782 16835 14 13 10663 18 20 3746 14 3 645 14 1990 27 206 16 14 4 29130 4 17512 13946 3\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "I0522 21:22:03.521989 140550058289024 create_pretraining_data.py:200] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "I0522 21:22:03.522072 140550058289024 create_pretraining_data.py:200] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:tensorflow:token_boundary: 1 0 1 1 0 1 1 1 0 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1\n",
      "I0522 21:22:03.522150 140550058289024 create_pretraining_data.py:200] token_boundary: 1 0 1 1 0 1 1 1 0 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1\n",
      "INFO:tensorflow:masked_lm_positions: 24 25 26 27 0 0\n",
      "I0522 21:22:03.522219 140550058289024 create_pretraining_data.py:200] masked_lm_positions: 24 25 26 27 0 0\n",
      "INFO:tensorflow:masked_lm_ids: 1267 99 14882 14599 0 0\n",
      "I0522 21:22:03.522302 140550058289024 create_pretraining_data.py:200] masked_lm_ids: 1267 99 14882 14599 0 0\n",
      "INFO:tensorflow:masked_lm_weights: 1.0 1.0 1.0 1.0 0.0 0.0\n",
      "I0522 21:22:03.522374 140550058289024 create_pretraining_data.py:200] masked_lm_weights: 1.0 1.0 1.0 1.0 0.0 0.0\n",
      "INFO:tensorflow:next_sentence_labels: 1\n",
      "I0522 21:22:03.522440 140550058289024 create_pretraining_data.py:200] next_sentence_labels: 1\n",
      "INFO:tensorflow:*** Example ***\n",
      "I0522 21:22:03.522704 140550058289024 create_pretraining_data.py:188] *** Example ***\n",
      "INFO:tensorflow:tokens: [CLS] ▁recordings ▁names ▁test ▁recordings ▁in ▁score [MASK] ▁trigger ▁re ▁enable ▁an ▁un link ed ▁ directional ▁link ▁dis able [MASK] ▁ [SEP] [MASK] ▁duration s ▁his t ogram [SEP]\n",
      "I0522 21:22:03.522817 140550058289024 create_pretraining_data.py:190] tokens: [CLS] ▁recordings ▁names ▁test ▁recordings ▁in ▁score [MASK] ▁trigger ▁re ▁enable ▁an ▁un link ed ▁ directional ▁link ▁dis able [MASK] ▁ [SEP] [MASK] ▁duration s ▁his t ogram [SEP]\n",
      "INFO:tensorflow:input_ids: 2 4655 1817 1289 4655 19 1618 4 7286 302 9240 40 367 6258 69 13 19393 3508 1460 579 4 13 3 4 9403 18 33 38 20476 3\n",
      "I0522 21:22:03.522913 140550058289024 create_pretraining_data.py:200] input_ids: 2 4655 1817 1289 4655 19 1618 4 7286 302 9240 40 367 6258 69 13 19393 3508 1460 579 4 13 3 4 9403 18 33 38 20476 3\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "I0522 21:22:03.522995 140550058289024 create_pretraining_data.py:200] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1\n",
      "I0522 21:22:03.523098 140550058289024 create_pretraining_data.py:200] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1\n",
      "INFO:tensorflow:token_boundary: 1 1 1 1 1 1 1 1 0 1 1 1 1 0 0 1 0 1 1 0 1 1 1 1 1 0 1 0 0 1\n",
      "I0522 21:22:03.523182 140550058289024 create_pretraining_data.py:200] token_boundary: 1 1 1 1 1 1 1 1 0 1 1 1 1 0 0 1 0 1 1 0 1 1 1 1 1 0 1 0 0 1\n",
      "INFO:tensorflow:masked_lm_positions: 7 8 20 23 0 0\n",
      "I0522 21:22:03.523278 140550058289024 create_pretraining_data.py:200] masked_lm_positions: 7 8 20 23 0 0\n",
      "INFO:tensorflow:masked_lm_ids: 1054 3554 21 14219 0 0\n",
      "I0522 21:22:03.523347 140550058289024 create_pretraining_data.py:200] masked_lm_ids: 1054 3554 21 14219 0 0\n",
      "INFO:tensorflow:masked_lm_weights: 1.0 1.0 1.0 1.0 0.0 0.0\n",
      "I0522 21:22:03.523417 140550058289024 create_pretraining_data.py:200] masked_lm_weights: 1.0 1.0 1.0 1.0 0.0 0.0\n",
      "INFO:tensorflow:next_sentence_labels: 1\n",
      "I0522 21:22:03.523483 140550058289024 create_pretraining_data.py:200] next_sentence_labels: 1\n",
      "INFO:tensorflow:*** Example ***\n",
      "I0522 21:22:03.523755 140550058289024 create_pretraining_data.py:188] *** Example ***\n",
      "INFO:tensorflow:tokens: [CLS] ▁a ▁memo ization ▁decor ator ▁borrowed ▁and ▁modified ▁from ▁http [SEP] ▁a ▁default ▁permission [MASK] [MASK] ▁a ▁new ▁origin ▁determine ▁whether ▁a [MASK] quest ▁matches ▁an ▁origin ▁pattern [SEP]\n",
      "I0522 21:22:03.523863 140550058289024 create_pretraining_data.py:190] tokens: [CLS] ▁a ▁memo ization ▁decor ator ▁borrowed ▁and ▁modified ▁from ▁http [SEP] ▁a ▁default ▁permission [MASK] [MASK] ▁a ▁new ▁origin ▁determine ▁whether ▁a [MASK] quest ▁matches ▁an ▁origin ▁pattern [SEP]\n",
      "INFO:tensorflow:input_ids: 2 21 22236 1829 17650 3457 13074 17 5372 37 7775 3 21 12838 5572 4 4 21 78 2986 3746 1472 21 4 10351 1717 40 2986 3732 3\n",
      "I0522 21:22:03.523963 140550058289024 create_pretraining_data.py:200] input_ids: 2 21 22236 1829 17650 3457 13074 17 5372 37 7775 3 21 12838 5572 4 4 21 78 2986 3746 1472 21 4 10351 1717 40 2986 3732 3\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "I0522 21:22:03.524048 140550058289024 create_pretraining_data.py:200] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "I0522 21:22:03.524132 140550058289024 create_pretraining_data.py:200] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:tensorflow:token_boundary: 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "I0522 21:22:03.524211 140550058289024 create_pretraining_data.py:200] token_boundary: 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:tensorflow:masked_lm_positions: 15 16 23 24 0 0\n",
      "I0522 21:22:03.524427 140550058289024 create_pretraining_data.py:200] masked_lm_positions: 15 16 23 24 0 0\n",
      "INFO:tensorflow:masked_lm_ids: 4851 26 504 2986 0 0\n",
      "I0522 21:22:03.524570 140550058289024 create_pretraining_data.py:200] masked_lm_ids: 4851 26 504 2986 0 0\n",
      "INFO:tensorflow:masked_lm_weights: 1.0 1.0 1.0 1.0 0.0 0.0\n",
      "I0522 21:22:03.524681 140550058289024 create_pretraining_data.py:200] masked_lm_weights: 1.0 1.0 1.0 1.0 0.0 0.0\n",
      "INFO:tensorflow:next_sentence_labels: 1\n",
      "I0522 21:22:03.524759 140550058289024 create_pretraining_data.py:200] next_sentence_labels: 1\n",
      "INFO:tensorflow:*** Example ***\n",
      "I0522 21:22:03.525084 140550058289024 create_pretraining_data.py:188] *** Example ***\n",
      "INFO:tensorflow:tokens: [CLS] ▁and ▁send ▁cloud form ation ▁custom [MASK] [MASK] [MASK] ▁a ▁function ▁to ▁add ▁exception ▁handling ▁and [SEP] ▁decorate ▁a ▁function ▁to ▁add ▁input ▁validation ▁for ▁resource ▁handler ▁functions [SEP]\n",
      "I0522 21:22:03.525203 140550058289024 create_pretraining_data.py:190] tokens: [CLS] ▁and ▁send ▁cloud form ation ▁custom [MASK] [MASK] [MASK] ▁a ▁function ▁to ▁add ▁exception ▁handling ▁and [SEP] ▁decorate ▁a ▁function ▁to ▁add ▁input ▁validation ▁for ▁resource ▁handler ▁functions [SEP]\n",
      "INFO:tensorflow:input_ids: 2 17 2660 4005 4190 857 5816 4 4 4 21 1990 20 3547 5391 7988 17 3 27117 21 1990 20 3547 6367 27999 26 6577 24641 3719 3\n",
      "I0522 21:22:03.525362 140550058289024 create_pretraining_data.py:200] input_ids: 2 17 2660 4005 4190 857 5816 4 4 4 21 1990 20 3547 5391 7988 17 3 27117 21 1990 20 3547 6367 27999 26 6577 24641 3719 3\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "I0522 21:22:03.525490 140550058289024 create_pretraining_data.py:200] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "I0522 21:22:03.525588 140550058289024 create_pretraining_data.py:200] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:tensorflow:token_boundary: 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "I0522 21:22:03.525691 140550058289024 create_pretraining_data.py:200] token_boundary: 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:tensorflow:masked_lm_positions: 7 8 9 24 0 0\n",
      "I0522 21:22:03.525806 140550058289024 create_pretraining_data.py:200] masked_lm_positions: 7 8 9 24 0 0\n",
      "INFO:tensorflow:masked_lm_ids: 6577 3916 27117 27999 0 0\n",
      "I0522 21:22:03.525890 140550058289024 create_pretraining_data.py:200] masked_lm_ids: 6577 3916 27117 27999 0 0\n",
      "INFO:tensorflow:masked_lm_weights: 1.0 1.0 1.0 1.0 0.0 0.0\n",
      "I0522 21:22:03.525968 140550058289024 create_pretraining_data.py:200] masked_lm_weights: 1.0 1.0 1.0 1.0 0.0 0.0\n",
      "INFO:tensorflow:next_sentence_labels: 1\n",
      "I0522 21:22:03.526041 140550058289024 create_pretraining_data.py:200] next_sentence_labels: 1\n",
      "INFO:tensorflow:*** Example ***\n",
      "I0522 21:22:03.526340 140550058289024 create_pretraining_data.py:188] *** Example ***\n",
      "INFO:tensorflow:tokens: [CLS] ▁to ▁create ▁a ▁category ▁raise ▁an ▁error [MASK] ▁the ▁seat ▁group ▁ s ▁and ▁the ▁ticket [MASK] [SEP] [MASK] ▁an ▁or ga ▁flag ▁for ▁a [MASK] ▁for ▁brand [SEP]\n",
      "I0522 21:22:03.526457 140550058289024 create_pretraining_data.py:190] tokens: [CLS] ▁to ▁create ▁a ▁category ▁raise ▁an ▁error [MASK] ▁the ▁seat ▁group ▁ s ▁and ▁the ▁ticket [MASK] [SEP] [MASK] ▁an ▁or ga ▁flag ▁for ▁a [MASK] ▁for ▁brand [SEP]\n",
      "INFO:tensorflow:input_ids: 2 20 1600 21 3230 3972 40 7019 4 14 988 214 13 18 17 14 6133 4 3 4 40 54 1136 3157 26 21 4 26 2209 3\n",
      "I0522 21:22:03.526560 140550058289024 create_pretraining_data.py:200] input_ids: 2 20 1600 21 3230 3972 40 7019 4 14 988 214 13 18 17 14 6133 4 3 4 40 54 1136 3157 26 21 4 26 2209 3\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "I0522 21:22:03.526650 140550058289024 create_pretraining_data.py:200] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1\n",
      "I0522 21:22:03.526739 140550058289024 create_pretraining_data.py:200] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:tensorflow:token_boundary: 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1\n",
      "I0522 21:22:03.526831 140550058289024 create_pretraining_data.py:200] token_boundary: 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1\n",
      "INFO:tensorflow:masked_lm_positions: 8 17 19 26 0 0\n",
      "I0522 21:22:03.526905 140550058289024 create_pretraining_data.py:200] masked_lm_positions: 8 17 19 26 0 0\n",
      "INFO:tensorflow:masked_lm_ids: 100 10194 1600 4155 0 0\n",
      "I0522 21:22:03.526978 140550058289024 create_pretraining_data.py:200] masked_lm_ids: 100 10194 1600 4155 0 0\n",
      "INFO:tensorflow:masked_lm_weights: 1.0 1.0 1.0 1.0 0.0 0.0\n",
      "I0522 21:22:03.618999 140550058289024 create_pretraining_data.py:200] masked_lm_weights: 1.0 1.0 1.0 1.0 0.0 0.0\n",
      "INFO:tensorflow:next_sentence_labels: 1\n",
      "I0522 21:22:03.619128 140550058289024 create_pretraining_data.py:200] next_sentence_labels: 1\n",
      "INFO:tensorflow:*** Example ***\n",
      "I0522 21:22:03.619575 140550058289024 create_pretraining_data.py:188] *** Example ***\n",
      "INFO:tensorflow:tokens: [CLS] ple ▁hell ▁albeit ▁a pi ▁server ▁u rl [MASK] ▁the [MASK] ▁path ▁of ▁a [SEP] ▁from ▁a ▁file ▁and ▁return ▁them ▁as ▁a ▁san it ized ▁list ▁non [SEP]\n",
      "I0522 21:22:03.619723 140550058289024 create_pretraining_data.py:190] tokens: [CLS] ple ▁hell ▁albeit ▁a pi ▁server ▁u rl [MASK] ▁the [MASK] ▁path ▁of ▁a [SEP] ▁from ▁a ▁file ▁and ▁return ▁them ▁as ▁a ▁san it ized ▁list ▁non [SEP]\n",
      "INFO:tensorflow:input_ids: 2 5106 1094 15080 21 2159 8128 287 6362 4 14 4 2013 16 21 3 37 21 3893 17 788 105 28 21 523 242 1333 968 538 3\n",
      "I0522 21:22:03.619864 140550058289024 create_pretraining_data.py:200] input_ids: 2 5106 1094 15080 21 2159 8128 287 6362 4 14 4 2013 16 21 3 37 21 3893 17 788 105 28 21 523 242 1333 968 538 3\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "I0522 21:22:03.619989 140550058289024 create_pretraining_data.py:200] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "I0522 21:22:03.620112 140550058289024 create_pretraining_data.py:200] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:tensorflow:token_boundary: 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1\n",
      "I0522 21:22:03.620227 140550058289024 create_pretraining_data.py:200] token_boundary: 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1\n",
      "INFO:tensorflow:masked_lm_positions: 2 3 9 11 0 0\n",
      "I0522 21:22:03.620360 140550058289024 create_pretraining_data.py:200] masked_lm_positions: 2 3 9 11 0 0\n",
      "INFO:tensorflow:masked_lm_ids: 26 40 4815 7070 0 0\n",
      "I0522 21:22:03.620464 140550058289024 create_pretraining_data.py:200] masked_lm_ids: 26 40 4815 7070 0 0\n",
      "INFO:tensorflow:masked_lm_weights: 1.0 1.0 1.0 1.0 0.0 0.0\n",
      "I0522 21:22:03.620568 140550058289024 create_pretraining_data.py:200] masked_lm_weights: 1.0 1.0 1.0 1.0 0.0 0.0\n",
      "INFO:tensorflow:next_sentence_labels: 0\n",
      "I0522 21:22:03.620671 140550058289024 create_pretraining_data.py:200] next_sentence_labels: 0\n",
      "INFO:tensorflow:*** Example ***\n",
      "I0522 21:22:03.621060 140550058289024 create_pretraining_data.py:188] *** Example ***\n",
      "INFO:tensorflow:tokens: [CLS] ▁string ▁ str ▁returns ▁without ▁de limit ers [MASK] ▁given ▁string [MASK] ▁bhopal ▁google [SEP] string ▁string ▁ str ▁returns ▁without ▁de limit ers ▁the [MASK] ▁string ▁compressed [SEP]\n",
      "I0522 21:22:03.621195 140550058289024 create_pretraining_data.py:190] tokens: [CLS] ▁string ▁ str ▁returns ▁without ▁de limit ers [MASK] ▁given ▁string [MASK] ▁bhopal ▁google [SEP] string ▁string ▁ str ▁returns ▁without ▁de limit ers ▁the [MASK] ▁string ▁compressed [SEP]\n",
      "INFO:tensorflow:input_ids: 2 3724 13 9729 4815 366 121 20565 445 4 504 3724 4 29255 8144 3 11130 3724 13 9729 4815 366 121 20565 445 14 4 3724 18472 3\n",
      "I0522 21:22:03.621325 140550058289024 create_pretraining_data.py:200] input_ids: 2 3724 13 9729 4815 366 121 20565 445 4 504 3724 4 29255 8144 3 11130 3724 13 9729 4815 366 121 20565 445 14 4 3724 18472 3\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "I0522 21:22:03.621415 140550058289024 create_pretraining_data.py:200] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "I0522 21:22:03.621505 140550058289024 create_pretraining_data.py:200] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:tensorflow:token_boundary: 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 0 1 1 1 0 0 1 1 1 1 1\n",
      "I0522 21:22:03.621585 140550058289024 create_pretraining_data.py:200] token_boundary: 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 0 1 1 1 0 0 1 1 1 1 1\n",
      "INFO:tensorflow:masked_lm_positions: 9 12 13 26 0 0\n",
      "I0522 21:22:03.621655 140550058289024 create_pretraining_data.py:200] masked_lm_positions: 9 12 13 26 0 0\n",
      "INFO:tensorflow:masked_lm_ids: 14 18472 568 504 0 0\n",
      "I0522 21:22:03.621725 140550058289024 create_pretraining_data.py:200] masked_lm_ids: 14 18472 568 504 0 0\n",
      "INFO:tensorflow:masked_lm_weights: 1.0 1.0 1.0 1.0 0.0 0.0\n",
      "I0522 21:22:03.621797 140550058289024 create_pretraining_data.py:200] masked_lm_weights: 1.0 1.0 1.0 1.0 0.0 0.0\n",
      "INFO:tensorflow:next_sentence_labels: 1\n",
      "I0522 21:22:03.621863 140550058289024 create_pretraining_data.py:200] next_sentence_labels: 1\n",
      "INFO:tensorflow:*** Example ***\n",
      "I0522 21:22:03.622133 140550058289024 create_pretraining_data.py:188] *** Example ***\n",
      "INFO:tensorflow:tokens: [CLS] ▁convert ▁the [MASK] [MASK] ▁into ▁a yn act ically ▁correct ▁ip table s ▁command [SEP] ▁pre defined ▁chains ▁aka ▁lists [MASK] [MASK] ▁as ▁new ▁here ▁put ▁line ▁into [SEP]\n",
      "I0522 21:22:03.622254 140550058289024 create_pretraining_data.py:190] tokens: [CLS] ▁convert ▁the [MASK] [MASK] ▁into ▁a yn act ically ▁correct ▁ip table s ▁command [SEP] ▁pre defined ▁chains ▁aka ▁lists [MASK] [MASK] ▁as ▁new ▁here ▁put ▁line ▁into [SEP]\n",
      "INFO:tensorflow:input_ids: 2 8406 14 4 4 77 21 4124 5183 8438 4456 15735 5924 18 1202 3 782 13439 8864 20892 7227 4 4 28 78 235 442 293 77 3\n",
      "I0522 21:22:03.622362 140550058289024 create_pretraining_data.py:200] input_ids: 2 8406 14 4 4 77 21 4124 5183 8438 4456 15735 5924 18 1202 3 782 13439 8864 20892 7227 4 4 28 78 235 442 293 77 3\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "I0522 21:22:03.622501 140550058289024 create_pretraining_data.py:200] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "I0522 21:22:03.622624 140550058289024 create_pretraining_data.py:200] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:tensorflow:token_boundary: 1 1 1 1 1 1 1 0 0 0 1 1 0 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "I0522 21:22:03.622716 140550058289024 create_pretraining_data.py:200] token_boundary: 1 1 1 1 1 1 1 0 0 0 1 1 0 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:tensorflow:masked_lm_positions: 3 4 21 22 0 0\n",
      "I0522 21:22:03.622793 140550058289024 create_pretraining_data.py:200] masked_lm_positions: 3 4 21 22 0 0\n",
      "INFO:tensorflow:masked_lm_ids: 1828 97 50 18161 0 0\n",
      "I0522 21:22:03.622863 140550058289024 create_pretraining_data.py:200] masked_lm_ids: 1828 97 50 18161 0 0\n",
      "INFO:tensorflow:masked_lm_weights: 1.0 1.0 1.0 1.0 0.0 0.0\n",
      "I0522 21:22:03.622934 140550058289024 create_pretraining_data.py:200] masked_lm_weights: 1.0 1.0 1.0 1.0 0.0 0.0\n",
      "INFO:tensorflow:next_sentence_labels: 1\n",
      "I0522 21:22:03.622999 140550058289024 create_pretraining_data.py:200] next_sentence_labels: 1\n",
      "INFO:tensorflow:Wrote 1720445 total instances\n",
      "I0522 21:26:36.006664 140550058289024 create_pretraining_data.py:205] Wrote 1720445 total instances\n"
     ]
    }
   ],
   "source": [
    "!python \"./create_pretraining_data.py\" \\\n",
    "  --input_file  \"./docstrings.txt\" \\\n",
    "  --output_file  \"./tf_examples\" \\\n",
    "  --vocab_file  \"./30k-clean.vocab\" \\\n",
    "  --spm_model_file \"./30k-clean.model\" \\\n",
    "  --do_lower_case True \\\n",
    "  --max_seq_length 50 \\\n",
    "  --max_predictions_per_seq 8 \\\n",
    "  --random_seed 12345 \\\n",
    "  --dupe_factor 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running incremental training to fine-tune ALBERT\n",
    "Once we have generated the training data from the above process we can begin the training process by running the run_pretraining.py file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "zJEkkWIMt53G",
    "outputId": "94d97e5f-1450-44e3-86b0-70d3887301a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/albert/lamb_optimizer.py:34: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/albert/modeling.py:116: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n",
      "W0523 13:42:04.451838 139802773264256 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/albert/modeling.py:116: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n",
      "INFO:tensorflow:*** Input Files ***\n",
      "I0523 13:42:04.454887 139802773264256 run_pretraining.py:484] *** Input Files ***\n",
      "INFO:tensorflow:  /content/drive/My Drive/BERT Training/tf_examples\n",
      "I0523 13:42:04.455017 139802773264256 run_pretraining.py:486]   /content/drive/My Drive/BERT Training/tf_examples\n",
      "WARNING:tensorflow:Estimator's model_fn (<function model_fn_builder.<locals>.model_fn at 0x7f25ec3b31e0>) includes params argument, but params are not passed to Estimator.\n",
      "W0523 13:42:04.455364 139802773264256 estimator.py:1994] Estimator's model_fn (<function model_fn_builder.<locals>.model_fn at 0x7f25ec3b31e0>) includes params argument, but params are not passed to Estimator.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/content/drive/My Drive/BERT Training/model', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 10000, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f25ec3b4c18>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=1000, num_shards=8, num_cores_per_replica=None, per_host_input_for_training=3, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2, experimental_host_call_every_n_steps=1), '_cluster': None}\n",
      "I0523 13:42:04.456349 139802773264256 estimator.py:212] Using config: {'_model_dir': '/content/drive/My Drive/BERT Training/model', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 10000, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f25ec3b4c18>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=1000, num_shards=8, num_cores_per_replica=None, per_host_input_for_training=3, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2, experimental_host_call_every_n_steps=1), '_cluster': None}\n",
      "INFO:tensorflow:_TPUContext: eval_on_tpu True\n",
      "I0523 13:42:04.456972 139802773264256 tpu_context.py:220] _TPUContext: eval_on_tpu True\n",
      "WARNING:tensorflow:eval_on_tpu ignored because use_tpu is False.\n",
      "W0523 13:42:04.457365 139802773264256 tpu_context.py:222] eval_on_tpu ignored because use_tpu is False.\n",
      "INFO:tensorflow:***** Running training *****\n",
      "I0523 13:42:04.457461 139802773264256 run_pretraining.py:527] ***** Running training *****\n",
      "INFO:tensorflow:  Batch size = 64\n",
      "I0523 13:42:04.457535 139802773264256 run_pretraining.py:528]   Batch size = 64\n",
      "INFO:tensorflow:Skipping training since max_steps has already saved.\n",
      "I0523 13:42:04.471787 139802773264256 estimator.py:363] Skipping training since max_steps has already saved.\n",
      "INFO:tensorflow:training_loop marked as finished\n",
      "I0523 13:42:04.471953 139802773264256 error_handling.py:101] training_loop marked as finished\n",
      "INFO:tensorflow:***** Running evaluation *****\n",
      "I0523 13:42:04.472070 139802773264256 run_pretraining.py:537] ***** Running evaluation *****\n",
      "INFO:tensorflow:  Batch size = 32\n",
      "I0523 13:42:04.472149 139802773264256 run_pretraining.py:538]   Batch size = 32\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "W0523 13:42:04.481830 139802773264256 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /content/drive/My Drive/BERT Training/run_pretraining.py:448: map_and_batch_with_legacy_function (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.experimental.map_and_batch()\n",
      "W0523 13:42:04.497659 139802773264256 deprecation.py:323] From /content/drive/My Drive/BERT Training/run_pretraining.py:448: map_and_batch_with_legacy_function (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.experimental.map_and_batch()\n",
      "WARNING:tensorflow:From /content/drive/My Drive/BERT Training/run_pretraining.py:464: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "W0523 13:42:04.634776 139802773264256 deprecation.py:323] From /content/drive/My Drive/BERT Training/run_pretraining.py:464: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "INFO:tensorflow:<DatasetV1Adapter shapes: {input_ids: (32, 30), input_mask: (32, 30), masked_lm_ids: (32, 6), masked_lm_positions: (32, 6), masked_lm_weights: (32, 6), next_sentence_labels: (32, 1), segment_ids: (32, 30)}, types: {input_ids: tf.int32, input_mask: tf.int32, masked_lm_ids: tf.int32, masked_lm_positions: tf.int32, masked_lm_weights: tf.float32, next_sentence_labels: tf.int32, segment_ids: tf.int32}>\n",
      "I0523 13:42:04.642251 139802773264256 run_pretraining.py:449] <DatasetV1Adapter shapes: {input_ids: (32, 30), input_mask: (32, 30), masked_lm_ids: (32, 6), masked_lm_positions: (32, 6), masked_lm_weights: (32, 6), next_sentence_labels: (32, 1), segment_ids: (32, 30)}, types: {input_ids: tf.int32, input_mask: tf.int32, masked_lm_ids: tf.int32, masked_lm_positions: tf.int32, masked_lm_weights: tf.float32, next_sentence_labels: tf.int32, segment_ids: tf.int32}>\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "I0523 13:42:04.649237 139802773264256 estimator.py:1148] Calling model_fn.\n",
      "INFO:tensorflow:Running eval on CPU\n",
      "I0523 13:42:04.649424 139802773264256 tpu_estimator.py:3124] Running eval on CPU\n",
      "INFO:tensorflow:*** Features ***\n",
      "I0523 13:42:04.649749 139802773264256 run_pretraining.py:141] *** Features ***\n",
      "INFO:tensorflow:  name = input_ids, shape = (32, 30)\n",
      "I0523 13:42:04.649885 139802773264256 run_pretraining.py:143]   name = input_ids, shape = (32, 30)\n",
      "INFO:tensorflow:  name = input_mask, shape = (32, 30)\n",
      "I0523 13:42:04.649988 139802773264256 run_pretraining.py:143]   name = input_mask, shape = (32, 30)\n",
      "INFO:tensorflow:  name = masked_lm_ids, shape = (32, 6)\n",
      "I0523 13:42:04.650084 139802773264256 run_pretraining.py:143]   name = masked_lm_ids, shape = (32, 6)\n",
      "INFO:tensorflow:  name = masked_lm_positions, shape = (32, 6)\n",
      "I0523 13:42:04.650187 139802773264256 run_pretraining.py:143]   name = masked_lm_positions, shape = (32, 6)\n",
      "INFO:tensorflow:  name = masked_lm_weights, shape = (32, 6)\n",
      "I0523 13:42:04.650284 139802773264256 run_pretraining.py:143]   name = masked_lm_weights, shape = (32, 6)\n",
      "INFO:tensorflow:  name = next_sentence_labels, shape = (32, 1)\n",
      "I0523 13:42:04.650380 139802773264256 run_pretraining.py:143]   name = next_sentence_labels, shape = (32, 1)\n",
      "INFO:tensorflow:  name = segment_ids, shape = (32, 30)\n",
      "I0523 13:42:04.650475 139802773264256 run_pretraining.py:143]   name = segment_ids, shape = (32, 30)\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/albert/modeling.py:194: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "W0523 13:42:04.650689 139802773264256 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/albert/modeling.py:194: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/albert/modeling.py:507: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "W0523 13:42:04.651866 139802773264256 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/albert/modeling.py:507: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/albert/modeling.py:588: The name tf.assert_less_equal is deprecated. Please use tf.compat.v1.assert_less_equal instead.\n",
      "\n",
      "W0523 13:42:04.668815 139802773264256 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/albert/modeling.py:588: The name tf.assert_less_equal is deprecated. Please use tf.compat.v1.assert_less_equal instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/albert/modeling.py:1025: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
      "\n",
      "W0523 13:42:04.701385 139802773264256 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/albert/modeling.py:1025: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/albert/modeling.py:253: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n",
      "W0523 13:42:05.588614 139802773264256 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/albert/modeling.py:253: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "W0523 13:42:05.590010 139802773264256 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "INFO:tensorflow:number of hidden group 1 to initialize\n",
      "I0523 13:42:05.668205 139802773264256 run_pretraining.py:186] number of hidden group 1 to initialize\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/albert/modeling.py:389: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
      "\n",
      "W0523 13:42:05.671734 139802773264256 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/albert/modeling.py:389: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
      "\n",
      "INFO:tensorflow:name bert/embeddings/word_embeddings match to bert/embeddings/word_embeddings\n",
      "I0523 13:42:05.671906 139802773264256 modeling.py:389] name bert/embeddings/word_embeddings match to bert/embeddings/word_embeddings\n",
      "INFO:tensorflow:name bert/embeddings/token_type_embeddings match to bert/embeddings/token_type_embeddings\n",
      "I0523 13:42:05.672005 139802773264256 modeling.py:389] name bert/embeddings/token_type_embeddings match to bert/embeddings/token_type_embeddings\n",
      "INFO:tensorflow:name bert/embeddings/position_embeddings match to bert/embeddings/position_embeddings\n",
      "I0523 13:42:05.672081 139802773264256 modeling.py:389] name bert/embeddings/position_embeddings match to bert/embeddings/position_embeddings\n",
      "INFO:tensorflow:name bert/embeddings/LayerNorm/beta match to bert/embeddings/LayerNorm/beta\n",
      "I0523 13:42:05.672151 139802773264256 modeling.py:389] name bert/embeddings/LayerNorm/beta match to bert/embeddings/LayerNorm/beta\n",
      "INFO:tensorflow:name bert/embeddings/LayerNorm/gamma match to bert/embeddings/LayerNorm/gamma\n",
      "I0523 13:42:05.672216 139802773264256 modeling.py:389] name bert/embeddings/LayerNorm/gamma match to bert/embeddings/LayerNorm/gamma\n",
      "INFO:tensorflow:name bert/encoder/embedding_hidden_mapping_in/kernel match to bert/encoder/embedding_hidden_mapping_in/kernel\n",
      "I0523 13:42:05.672280 139802773264256 modeling.py:389] name bert/encoder/embedding_hidden_mapping_in/kernel match to bert/encoder/embedding_hidden_mapping_in/kernel\n",
      "INFO:tensorflow:name bert/encoder/embedding_hidden_mapping_in/bias match to bert/encoder/embedding_hidden_mapping_in/bias\n",
      "I0523 13:42:05.672345 139802773264256 modeling.py:389] name bert/encoder/embedding_hidden_mapping_in/bias match to bert/encoder/embedding_hidden_mapping_in/bias\n",
      "INFO:tensorflow:name bert/encoder/transformer/group_0/inner_group_0/attention_1/self/query/kernel match to bert/encoder/transformer/group_0/inner_group_0/attention_1/self/query/kernel\n",
      "I0523 13:42:05.672409 139802773264256 modeling.py:389] name bert/encoder/transformer/group_0/inner_group_0/attention_1/self/query/kernel match to bert/encoder/transformer/group_0/inner_group_0/attention_1/self/query/kernel\n",
      "INFO:tensorflow:name bert/encoder/transformer/group_0/inner_group_0/attention_1/self/query/bias match to bert/encoder/transformer/group_0/inner_group_0/attention_1/self/query/bias\n",
      "I0523 13:42:05.672474 139802773264256 modeling.py:389] name bert/encoder/transformer/group_0/inner_group_0/attention_1/self/query/bias match to bert/encoder/transformer/group_0/inner_group_0/attention_1/self/query/bias\n",
      "INFO:tensorflow:name bert/encoder/transformer/group_0/inner_group_0/attention_1/self/key/kernel match to bert/encoder/transformer/group_0/inner_group_0/attention_1/self/key/kernel\n",
      "I0523 13:42:05.672537 139802773264256 modeling.py:389] name bert/encoder/transformer/group_0/inner_group_0/attention_1/self/key/kernel match to bert/encoder/transformer/group_0/inner_group_0/attention_1/self/key/kernel\n",
      "INFO:tensorflow:name bert/encoder/transformer/group_0/inner_group_0/attention_1/self/key/bias match to bert/encoder/transformer/group_0/inner_group_0/attention_1/self/key/bias\n",
      "I0523 13:42:05.672626 139802773264256 modeling.py:389] name bert/encoder/transformer/group_0/inner_group_0/attention_1/self/key/bias match to bert/encoder/transformer/group_0/inner_group_0/attention_1/self/key/bias\n",
      "INFO:tensorflow:name bert/encoder/transformer/group_0/inner_group_0/attention_1/self/value/kernel match to bert/encoder/transformer/group_0/inner_group_0/attention_1/self/value/kernel\n",
      "I0523 13:42:05.672703 139802773264256 modeling.py:389] name bert/encoder/transformer/group_0/inner_group_0/attention_1/self/value/kernel match to bert/encoder/transformer/group_0/inner_group_0/attention_1/self/value/kernel\n",
      "INFO:tensorflow:name bert/encoder/transformer/group_0/inner_group_0/attention_1/self/value/bias match to bert/encoder/transformer/group_0/inner_group_0/attention_1/self/value/bias\n",
      "I0523 13:42:05.672767 139802773264256 modeling.py:389] name bert/encoder/transformer/group_0/inner_group_0/attention_1/self/value/bias match to bert/encoder/transformer/group_0/inner_group_0/attention_1/self/value/bias\n",
      "INFO:tensorflow:name bert/encoder/transformer/group_0/inner_group_0/attention_1/output/dense/kernel match to bert/encoder/transformer/group_0/inner_group_0/attention_1/output/dense/kernel\n",
      "I0523 13:42:05.672831 139802773264256 modeling.py:389] name bert/encoder/transformer/group_0/inner_group_0/attention_1/output/dense/kernel match to bert/encoder/transformer/group_0/inner_group_0/attention_1/output/dense/kernel\n",
      "INFO:tensorflow:name bert/encoder/transformer/group_0/inner_group_0/attention_1/output/dense/bias match to bert/encoder/transformer/group_0/inner_group_0/attention_1/output/dense/bias\n",
      "I0523 13:42:05.672895 139802773264256 modeling.py:389] name bert/encoder/transformer/group_0/inner_group_0/attention_1/output/dense/bias match to bert/encoder/transformer/group_0/inner_group_0/attention_1/output/dense/bias\n",
      "INFO:tensorflow:name bert/encoder/transformer/group_0/inner_group_0/LayerNorm/beta match to bert/encoder/transformer/group_0/inner_group_0/LayerNorm/beta\n",
      "I0523 13:42:05.672960 139802773264256 modeling.py:389] name bert/encoder/transformer/group_0/inner_group_0/LayerNorm/beta match to bert/encoder/transformer/group_0/inner_group_0/LayerNorm/beta\n",
      "INFO:tensorflow:name bert/encoder/transformer/group_0/inner_group_0/LayerNorm/gamma match to bert/encoder/transformer/group_0/inner_group_0/LayerNorm/gamma\n",
      "I0523 13:42:05.673044 139802773264256 modeling.py:389] name bert/encoder/transformer/group_0/inner_group_0/LayerNorm/gamma match to bert/encoder/transformer/group_0/inner_group_0/LayerNorm/gamma\n",
      "INFO:tensorflow:name bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/dense/kernel match to bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/dense/kernel\n",
      "I0523 13:42:05.673109 139802773264256 modeling.py:389] name bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/dense/kernel match to bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/dense/kernel\n",
      "INFO:tensorflow:name bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/dense/bias match to bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/dense/bias\n",
      "I0523 13:42:05.673174 139802773264256 modeling.py:389] name bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/dense/bias match to bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/dense/bias\n",
      "INFO:tensorflow:name bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/output/dense/kernel match to bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/output/dense/kernel\n",
      "I0523 13:42:05.673238 139802773264256 modeling.py:389] name bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/output/dense/kernel match to bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/output/dense/kernel\n",
      "INFO:tensorflow:name bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/output/dense/bias match to bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/output/dense/bias\n",
      "I0523 13:42:05.673304 139802773264256 modeling.py:389] name bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/output/dense/bias match to bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/output/dense/bias\n",
      "INFO:tensorflow:name bert/encoder/transformer/group_0/inner_group_0/LayerNorm_1/beta match to bert/encoder/transformer/group_0/inner_group_0/LayerNorm_1/beta\n",
      "I0523 13:42:05.673368 139802773264256 modeling.py:389] name bert/encoder/transformer/group_0/inner_group_0/LayerNorm_1/beta match to bert/encoder/transformer/group_0/inner_group_0/LayerNorm_1/beta\n",
      "INFO:tensorflow:name bert/encoder/transformer/group_0/inner_group_0/LayerNorm_1/gamma match to bert/encoder/transformer/group_0/inner_group_0/LayerNorm_1/gamma\n",
      "I0523 13:42:05.673436 139802773264256 modeling.py:389] name bert/encoder/transformer/group_0/inner_group_0/LayerNorm_1/gamma match to bert/encoder/transformer/group_0/inner_group_0/LayerNorm_1/gamma\n",
      "INFO:tensorflow:name bert/pooler/dense/kernel match to bert/pooler/dense/kernel\n",
      "I0523 13:42:05.673501 139802773264256 modeling.py:389] name bert/pooler/dense/kernel match to bert/pooler/dense/kernel\n",
      "INFO:tensorflow:name bert/pooler/dense/bias match to bert/pooler/dense/bias\n",
      "I0523 13:42:05.673565 139802773264256 modeling.py:389] name bert/pooler/dense/bias match to bert/pooler/dense/bias\n",
      "INFO:tensorflow:name cls/predictions/transform/dense/kernel match to cls/predictions/transform/dense/kernel\n",
      "I0523 13:42:05.673658 139802773264256 modeling.py:389] name cls/predictions/transform/dense/kernel match to cls/predictions/transform/dense/kernel\n",
      "INFO:tensorflow:name cls/predictions/transform/dense/bias match to cls/predictions/transform/dense/bias\n",
      "I0523 13:42:05.673723 139802773264256 modeling.py:389] name cls/predictions/transform/dense/bias match to cls/predictions/transform/dense/bias\n",
      "INFO:tensorflow:name cls/predictions/transform/LayerNorm/beta match to cls/predictions/transform/LayerNorm/beta\n",
      "I0523 13:42:05.673787 139802773264256 modeling.py:389] name cls/predictions/transform/LayerNorm/beta match to cls/predictions/transform/LayerNorm/beta\n",
      "INFO:tensorflow:name cls/predictions/transform/LayerNorm/gamma match to cls/predictions/transform/LayerNorm/gamma\n",
      "I0523 13:42:05.673851 139802773264256 modeling.py:389] name cls/predictions/transform/LayerNorm/gamma match to cls/predictions/transform/LayerNorm/gamma\n",
      "INFO:tensorflow:name cls/predictions/output_bias match to cls/predictions/output_bias\n",
      "I0523 13:42:05.673915 139802773264256 modeling.py:389] name cls/predictions/output_bias match to cls/predictions/output_bias\n",
      "INFO:tensorflow:name cls/seq_relationship/output_weights match to cls/seq_relationship/output_weights\n",
      "I0523 13:42:05.673980 139802773264256 modeling.py:389] name cls/seq_relationship/output_weights match to cls/seq_relationship/output_weights\n",
      "INFO:tensorflow:name cls/seq_relationship/output_bias match to cls/seq_relationship/output_bias\n",
      "I0523 13:42:05.674044 139802773264256 modeling.py:389] name cls/seq_relationship/output_bias match to cls/seq_relationship/output_bias\n",
      "INFO:tensorflow:initialize the 0th layer\n",
      "I0523 13:42:05.674117 139802773264256 run_pretraining.py:207] initialize the 0th layer\n",
      "INFO:tensorflow:OrderedDict([('bert/embeddings/word_embeddings', 'bert/embeddings/word_embeddings'), ('bert/embeddings/token_type_embeddings', 'bert/embeddings/token_type_embeddings'), ('bert/embeddings/position_embeddings', 'bert/embeddings/position_embeddings'), ('bert/embeddings/LayerNorm/beta', 'bert/embeddings/LayerNorm/beta'), ('bert/embeddings/LayerNorm/gamma', 'bert/embeddings/LayerNorm/gamma'), ('bert/encoder/embedding_hidden_mapping_in/kernel', 'bert/encoder/embedding_hidden_mapping_in/kernel'), ('bert/encoder/embedding_hidden_mapping_in/bias', 'bert/encoder/embedding_hidden_mapping_in/bias'), ('bert/encoder/transformer/group_0/inner_group_0/attention_1/self/query/kernel', 'bert/encoder/transformer/group_0/inner_group_0/attention_1/self/query/kernel'), ('bert/encoder/transformer/group_0/inner_group_0/attention_1/self/query/bias', 'bert/encoder/transformer/group_0/inner_group_0/attention_1/self/query/bias'), ('bert/encoder/transformer/group_0/inner_group_0/attention_1/self/key/kernel', 'bert/encoder/transformer/group_0/inner_group_0/attention_1/self/key/kernel'), ('bert/encoder/transformer/group_0/inner_group_0/attention_1/self/key/bias', 'bert/encoder/transformer/group_0/inner_group_0/attention_1/self/key/bias'), ('bert/encoder/transformer/group_0/inner_group_0/attention_1/self/value/kernel', 'bert/encoder/transformer/group_0/inner_group_0/attention_1/self/value/kernel'), ('bert/encoder/transformer/group_0/inner_group_0/attention_1/self/value/bias', 'bert/encoder/transformer/group_0/inner_group_0/attention_1/self/value/bias'), ('bert/encoder/transformer/group_0/inner_group_0/attention_1/output/dense/kernel', 'bert/encoder/transformer/group_0/inner_group_0/attention_1/output/dense/kernel'), ('bert/encoder/transformer/group_0/inner_group_0/attention_1/output/dense/bias', 'bert/encoder/transformer/group_0/inner_group_0/attention_1/output/dense/bias'), ('bert/encoder/transformer/group_0/inner_group_0/LayerNorm/beta', 'bert/encoder/transformer/group_0/inner_group_0/LayerNorm/beta'), ('bert/encoder/transformer/group_0/inner_group_0/LayerNorm/gamma', 'bert/encoder/transformer/group_0/inner_group_0/LayerNorm/gamma'), ('bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/dense/kernel', 'bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/dense/kernel'), ('bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/dense/bias', 'bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/dense/bias'), ('bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/output/dense/kernel', 'bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/output/dense/kernel'), ('bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/output/dense/bias', 'bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/output/dense/bias'), ('bert/encoder/transformer/group_0/inner_group_0/LayerNorm_1/beta', 'bert/encoder/transformer/group_0/inner_group_0/LayerNorm_1/beta'), ('bert/encoder/transformer/group_0/inner_group_0/LayerNorm_1/gamma', 'bert/encoder/transformer/group_0/inner_group_0/LayerNorm_1/gamma'), ('bert/pooler/dense/kernel', 'bert/pooler/dense/kernel'), ('bert/pooler/dense/bias', 'bert/pooler/dense/bias'), ('cls/predictions/transform/dense/kernel', 'cls/predictions/transform/dense/kernel'), ('cls/predictions/transform/dense/bias', 'cls/predictions/transform/dense/bias'), ('cls/predictions/transform/LayerNorm/beta', 'cls/predictions/transform/LayerNorm/beta'), ('cls/predictions/transform/LayerNorm/gamma', 'cls/predictions/transform/LayerNorm/gamma'), ('cls/predictions/output_bias', 'cls/predictions/output_bias'), ('cls/seq_relationship/output_weights', 'cls/seq_relationship/output_weights'), ('cls/seq_relationship/output_bias', 'cls/seq_relationship/output_bias')])\n",
      "I0523 13:42:05.674189 139802773264256 run_pretraining.py:208] OrderedDict([('bert/embeddings/word_embeddings', 'bert/embeddings/word_embeddings'), ('bert/embeddings/token_type_embeddings', 'bert/embeddings/token_type_embeddings'), ('bert/embeddings/position_embeddings', 'bert/embeddings/position_embeddings'), ('bert/embeddings/LayerNorm/beta', 'bert/embeddings/LayerNorm/beta'), ('bert/embeddings/LayerNorm/gamma', 'bert/embeddings/LayerNorm/gamma'), ('bert/encoder/embedding_hidden_mapping_in/kernel', 'bert/encoder/embedding_hidden_mapping_in/kernel'), ('bert/encoder/embedding_hidden_mapping_in/bias', 'bert/encoder/embedding_hidden_mapping_in/bias'), ('bert/encoder/transformer/group_0/inner_group_0/attention_1/self/query/kernel', 'bert/encoder/transformer/group_0/inner_group_0/attention_1/self/query/kernel'), ('bert/encoder/transformer/group_0/inner_group_0/attention_1/self/query/bias', 'bert/encoder/transformer/group_0/inner_group_0/attention_1/self/query/bias'), ('bert/encoder/transformer/group_0/inner_group_0/attention_1/self/key/kernel', 'bert/encoder/transformer/group_0/inner_group_0/attention_1/self/key/kernel'), ('bert/encoder/transformer/group_0/inner_group_0/attention_1/self/key/bias', 'bert/encoder/transformer/group_0/inner_group_0/attention_1/self/key/bias'), ('bert/encoder/transformer/group_0/inner_group_0/attention_1/self/value/kernel', 'bert/encoder/transformer/group_0/inner_group_0/attention_1/self/value/kernel'), ('bert/encoder/transformer/group_0/inner_group_0/attention_1/self/value/bias', 'bert/encoder/transformer/group_0/inner_group_0/attention_1/self/value/bias'), ('bert/encoder/transformer/group_0/inner_group_0/attention_1/output/dense/kernel', 'bert/encoder/transformer/group_0/inner_group_0/attention_1/output/dense/kernel'), ('bert/encoder/transformer/group_0/inner_group_0/attention_1/output/dense/bias', 'bert/encoder/transformer/group_0/inner_group_0/attention_1/output/dense/bias'), ('bert/encoder/transformer/group_0/inner_group_0/LayerNorm/beta', 'bert/encoder/transformer/group_0/inner_group_0/LayerNorm/beta'), ('bert/encoder/transformer/group_0/inner_group_0/LayerNorm/gamma', 'bert/encoder/transformer/group_0/inner_group_0/LayerNorm/gamma'), ('bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/dense/kernel', 'bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/dense/kernel'), ('bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/dense/bias', 'bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/dense/bias'), ('bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/output/dense/kernel', 'bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/output/dense/kernel'), ('bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/output/dense/bias', 'bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/output/dense/bias'), ('bert/encoder/transformer/group_0/inner_group_0/LayerNorm_1/beta', 'bert/encoder/transformer/group_0/inner_group_0/LayerNorm_1/beta'), ('bert/encoder/transformer/group_0/inner_group_0/LayerNorm_1/gamma', 'bert/encoder/transformer/group_0/inner_group_0/LayerNorm_1/gamma'), ('bert/pooler/dense/kernel', 'bert/pooler/dense/kernel'), ('bert/pooler/dense/bias', 'bert/pooler/dense/bias'), ('cls/predictions/transform/dense/kernel', 'cls/predictions/transform/dense/kernel'), ('cls/predictions/transform/dense/bias', 'cls/predictions/transform/dense/bias'), ('cls/predictions/transform/LayerNorm/beta', 'cls/predictions/transform/LayerNorm/beta'), ('cls/predictions/transform/LayerNorm/gamma', 'cls/predictions/transform/LayerNorm/gamma'), ('cls/predictions/output_bias', 'cls/predictions/output_bias'), ('cls/seq_relationship/output_weights', 'cls/seq_relationship/output_weights'), ('cls/seq_relationship/output_bias', 'cls/seq_relationship/output_bias')])\n",
      "INFO:tensorflow:**** Trainable Variables ****\n",
      "I0523 13:42:05.825253 139802773264256 run_pretraining.py:211] **** Trainable Variables ****\n",
      "INFO:tensorflow:  name = bert/embeddings/word_embeddings:0, shape = (30000, 128), *INIT_FROM_CKPT*\n",
      "I0523 13:42:05.825485 139802773264256 run_pretraining.py:217]   name = bert/embeddings/word_embeddings:0, shape = (30000, 128), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/embeddings/token_type_embeddings:0, shape = (2, 128), *INIT_FROM_CKPT*\n",
      "I0523 13:42:05.825656 139802773264256 run_pretraining.py:217]   name = bert/embeddings/token_type_embeddings:0, shape = (2, 128), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/embeddings/position_embeddings:0, shape = (512, 128), *INIT_FROM_CKPT*\n",
      "I0523 13:42:05.825775 139802773264256 run_pretraining.py:217]   name = bert/embeddings/position_embeddings:0, shape = (512, 128), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/embeddings/LayerNorm/beta:0, shape = (128,), *INIT_FROM_CKPT*\n",
      "I0523 13:42:05.825864 139802773264256 run_pretraining.py:217]   name = bert/embeddings/LayerNorm/beta:0, shape = (128,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/embeddings/LayerNorm/gamma:0, shape = (128,), *INIT_FROM_CKPT*\n",
      "I0523 13:42:05.825945 139802773264256 run_pretraining.py:217]   name = bert/embeddings/LayerNorm/gamma:0, shape = (128,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/embedding_hidden_mapping_in/kernel:0, shape = (128, 768), *INIT_FROM_CKPT*\n",
      "I0523 13:42:05.826023 139802773264256 run_pretraining.py:217]   name = bert/encoder/embedding_hidden_mapping_in/kernel:0, shape = (128, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/embedding_hidden_mapping_in/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I0523 13:42:05.826113 139802773264256 run_pretraining.py:217]   name = bert/encoder/embedding_hidden_mapping_in/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/transformer/group_0/inner_group_0/attention_1/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I0523 13:42:05.826191 139802773264256 run_pretraining.py:217]   name = bert/encoder/transformer/group_0/inner_group_0/attention_1/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/transformer/group_0/inner_group_0/attention_1/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I0523 13:42:05.826282 139802773264256 run_pretraining.py:217]   name = bert/encoder/transformer/group_0/inner_group_0/attention_1/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/transformer/group_0/inner_group_0/attention_1/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I0523 13:42:05.826361 139802773264256 run_pretraining.py:217]   name = bert/encoder/transformer/group_0/inner_group_0/attention_1/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/transformer/group_0/inner_group_0/attention_1/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I0523 13:42:05.826443 139802773264256 run_pretraining.py:217]   name = bert/encoder/transformer/group_0/inner_group_0/attention_1/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/transformer/group_0/inner_group_0/attention_1/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I0523 13:42:05.826519 139802773264256 run_pretraining.py:217]   name = bert/encoder/transformer/group_0/inner_group_0/attention_1/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/transformer/group_0/inner_group_0/attention_1/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I0523 13:42:05.826624 139802773264256 run_pretraining.py:217]   name = bert/encoder/transformer/group_0/inner_group_0/attention_1/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/transformer/group_0/inner_group_0/attention_1/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I0523 13:42:05.826702 139802773264256 run_pretraining.py:217]   name = bert/encoder/transformer/group_0/inner_group_0/attention_1/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/transformer/group_0/inner_group_0/attention_1/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I0523 13:42:05.826784 139802773264256 run_pretraining.py:217]   name = bert/encoder/transformer/group_0/inner_group_0/attention_1/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/transformer/group_0/inner_group_0/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I0523 13:42:05.826860 139802773264256 run_pretraining.py:217]   name = bert/encoder/transformer/group_0/inner_group_0/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/transformer/group_0/inner_group_0/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I0523 13:42:05.826937 139802773264256 run_pretraining.py:217]   name = bert/encoder/transformer/group_0/inner_group_0/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "I0523 13:42:05.827013 139802773264256 run_pretraining.py:217]   name = bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "I0523 13:42:05.827105 139802773264256 run_pretraining.py:217]   name = bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "I0523 13:42:05.827188 139802773264256 run_pretraining.py:217]   name = bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I0523 13:42:05.827271 139802773264256 run_pretraining.py:217]   name = bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/transformer/group_0/inner_group_0/LayerNorm_1/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I0523 13:42:05.827348 139802773264256 run_pretraining.py:217]   name = bert/encoder/transformer/group_0/inner_group_0/LayerNorm_1/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/transformer/group_0/inner_group_0/LayerNorm_1/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I0523 13:42:05.827435 139802773264256 run_pretraining.py:217]   name = bert/encoder/transformer/group_0/inner_group_0/LayerNorm_1/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/pooler/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I0523 13:42:05.827514 139802773264256 run_pretraining.py:217]   name = bert/pooler/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/pooler/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I0523 13:42:05.827612 139802773264256 run_pretraining.py:217]   name = bert/pooler/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = cls/predictions/transform/dense/kernel:0, shape = (768, 128), *INIT_FROM_CKPT*\n",
      "I0523 13:42:05.827691 139802773264256 run_pretraining.py:217]   name = cls/predictions/transform/dense/kernel:0, shape = (768, 128), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = cls/predictions/transform/dense/bias:0, shape = (128,), *INIT_FROM_CKPT*\n",
      "I0523 13:42:05.827773 139802773264256 run_pretraining.py:217]   name = cls/predictions/transform/dense/bias:0, shape = (128,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = cls/predictions/transform/LayerNorm/beta:0, shape = (128,), *INIT_FROM_CKPT*\n",
      "I0523 13:42:05.827848 139802773264256 run_pretraining.py:217]   name = cls/predictions/transform/LayerNorm/beta:0, shape = (128,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = cls/predictions/transform/LayerNorm/gamma:0, shape = (128,), *INIT_FROM_CKPT*\n",
      "I0523 13:42:05.827924 139802773264256 run_pretraining.py:217]   name = cls/predictions/transform/LayerNorm/gamma:0, shape = (128,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = cls/predictions/output_bias:0, shape = (30000,), *INIT_FROM_CKPT*\n",
      "I0523 13:42:05.827999 139802773264256 run_pretraining.py:217]   name = cls/predictions/output_bias:0, shape = (30000,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = cls/seq_relationship/output_weights:0, shape = (2, 768), *INIT_FROM_CKPT*\n",
      "I0523 13:42:05.828074 139802773264256 run_pretraining.py:217]   name = cls/seq_relationship/output_weights:0, shape = (2, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = cls/seq_relationship/output_bias:0, shape = (2,), *INIT_FROM_CKPT*\n",
      "I0523 13:42:05.828177 139802773264256 run_pretraining.py:217]   name = cls/seq_relationship/output_bias:0, shape = (2,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "I0523 13:42:05.892161 139802773264256 estimator.py:1150] Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2020-05-23T13:42:05Z\n",
      "I0523 13:42:05.909027 139802773264256 evaluation.py:255] Starting evaluation at 2020-05-23T13:42:05Z\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/array_ops.py:1475: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0523 13:42:05.952023 139802773264256 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/array_ops.py:1475: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "I0523 13:42:06.018428 139802773264256 monitored_session.py:240] Graph was finalized.\n",
      "2020-05-23 13:42:06.018775: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2020-05-23 13:42:06.023435: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz\n",
      "2020-05-23 13:42:06.023650: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x31c9100 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2020-05-23 13:42:06.023679: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2020-05-23 13:42:06.025522: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
      "2020-05-23 13:42:06.147354: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-05-23 13:42:06.148104: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x31c92c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2020-05-23 13:42:06.148141: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla P4, Compute Capability 6.1\n",
      "2020-05-23 13:42:06.148306: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-05-23 13:42:06.148674: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
      "name: Tesla P4 major: 6 minor: 1 memoryClockRate(GHz): 1.1135\n",
      "pciBusID: 0000:00:04.0\n",
      "2020-05-23 13:42:06.149012: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
      "2020-05-23 13:42:06.150187: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
      "2020-05-23 13:42:06.151774: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\n",
      "2020-05-23 13:42:06.152179: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\n",
      "2020-05-23 13:42:06.153514: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\n",
      "2020-05-23 13:42:06.154428: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\n",
      "2020-05-23 13:42:06.157435: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
      "2020-05-23 13:42:06.157543: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-05-23 13:42:06.157940: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-05-23 13:42:06.158275: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
      "2020-05-23 13:42:06.158333: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
      "2020-05-23 13:42:06.159283: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2020-05-23 13:42:06.159309: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
      "2020-05-23 13:42:06.159319: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n",
      "2020-05-23 13:42:06.159422: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-05-23 13:42:06.159805: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-05-23 13:42:06.160138: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2020-05-23 13:42:06.160177: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7123 MB memory) -> physical GPU (device: 0, name: Tesla P4, pci bus id: 0000:00:04.0, compute capability: 6.1)\n",
      "INFO:tensorflow:Restoring parameters from /content/drive/My Drive/BERT Training/model/model.ckpt-100000\n",
      "I0523 13:42:06.162758 139802773264256 saver.py:1284] Restoring parameters from /content/drive/My Drive/BERT Training/model/model.ckpt-100000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "I0523 13:42:07.012295 139802773264256 session_manager.py:500] Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "I0523 13:42:07.036862 139802773264256 session_manager.py:502] Done running local_init_op.\n",
      "2020-05-23 13:42:07.350849: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
      "INFO:tensorflow:Evaluation [10/100]\n",
      "I0523 13:42:08.599204 139802773264256 evaluation.py:167] Evaluation [10/100]\n",
      "INFO:tensorflow:Evaluation [20/100]\n",
      "I0523 13:42:09.501199 139802773264256 evaluation.py:167] Evaluation [20/100]\n",
      "INFO:tensorflow:Evaluation [30/100]\n",
      "I0523 13:42:10.405273 139802773264256 evaluation.py:167] Evaluation [30/100]\n",
      "INFO:tensorflow:Evaluation [40/100]\n",
      "I0523 13:42:11.307370 139802773264256 evaluation.py:167] Evaluation [40/100]\n",
      "INFO:tensorflow:Evaluation [50/100]\n",
      "I0523 13:42:12.212860 139802773264256 evaluation.py:167] Evaluation [50/100]\n",
      "INFO:tensorflow:Evaluation [60/100]\n",
      "I0523 13:42:13.117780 139802773264256 evaluation.py:167] Evaluation [60/100]\n",
      "INFO:tensorflow:Evaluation [70/100]\n",
      "I0523 13:42:14.020228 139802773264256 evaluation.py:167] Evaluation [70/100]\n",
      "INFO:tensorflow:Evaluation [80/100]\n",
      "I0523 13:42:14.922837 139802773264256 evaluation.py:167] Evaluation [80/100]\n",
      "INFO:tensorflow:Evaluation [90/100]\n",
      "I0523 13:42:15.830289 139802773264256 evaluation.py:167] Evaluation [90/100]\n",
      "INFO:tensorflow:Evaluation [100/100]\n",
      "I0523 13:42:16.732255 139802773264256 evaluation.py:167] Evaluation [100/100]\n",
      "INFO:tensorflow:Finished evaluation at 2020-05-23-13:42:16\n",
      "I0523 13:42:16.779155 139802773264256 evaluation.py:275] Finished evaluation at 2020-05-23-13:42:16\n",
      "INFO:tensorflow:Saving dict for global step 100000: global_step = 100000, loss = 3.6782873, masked_lm_accuracy = 0.43784672, masked_lm_loss = 3.1814866, sentence_order_accuracy = 0.7240625, sentence_order_loss = 0.49681297\n",
      "I0523 13:42:16.779442 139802773264256 estimator.py:2049] Saving dict for global step 100000: global_step = 100000, loss = 3.6782873, masked_lm_accuracy = 0.43784672, masked_lm_loss = 3.1814866, sentence_order_accuracy = 0.7240625, sentence_order_loss = 0.49681297\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 100000: /content/drive/My Drive/BERT Training/model/model.ckpt-100000\n",
      "I0523 13:42:17.011036 139802773264256 estimator.py:2109] Saving 'checkpoint_path' summary for global step 100000: /content/drive/My Drive/BERT Training/model/model.ckpt-100000\n",
      "INFO:tensorflow:evaluation_loop marked as finished\n",
      "I0523 13:42:17.011862 139802773264256 error_handling.py:101] evaluation_loop marked as finished\n",
      "INFO:tensorflow:***** Eval results *****\n",
      "I0523 13:42:17.012033 139802773264256 run_pretraining.py:557] ***** Eval results *****\n",
      "INFO:tensorflow:  global_step = 100000\n",
      "I0523 13:42:17.015720 139802773264256 run_pretraining.py:560]   global_step = 100000\n",
      "INFO:tensorflow:saving /content/drive/My Drive/BERT Training/model/model.ckpt-100000.meta to /content/drive/My Drive/BERT Training/model/model.ckpt-best.meta\n",
      "I0523 13:42:17.269936 139802773264256 run_pretraining.py:568] saving /content/drive/My Drive/BERT Training/model/model.ckpt-100000.meta to /content/drive/My Drive/BERT Training/model/model.ckpt-best.meta\n",
      "INFO:tensorflow:saving /content/drive/My Drive/BERT Training/model/model.ckpt-100000.data-00000-of-00001 to /content/drive/My Drive/BERT Training/model/model.ckpt-best.data-00000-of-00001\n",
      "I0523 13:42:17.675872 139802773264256 run_pretraining.py:568] saving /content/drive/My Drive/BERT Training/model/model.ckpt-100000.data-00000-of-00001 to /content/drive/My Drive/BERT Training/model/model.ckpt-best.data-00000-of-00001\n",
      "INFO:tensorflow:saving /content/drive/My Drive/BERT Training/model/model.ckpt-100000.index to /content/drive/My Drive/BERT Training/model/model.ckpt-best.index\n",
      "I0523 13:42:21.925680 139802773264256 run_pretraining.py:568] saving /content/drive/My Drive/BERT Training/model/model.ckpt-100000.index to /content/drive/My Drive/BERT Training/model/model.ckpt-best.index\n",
      "INFO:tensorflow:  loss = 3.6782873\n",
      "I0523 13:42:22.117718 139802773264256 run_pretraining.py:560]   loss = 3.6782873\n",
      "INFO:tensorflow:  masked_lm_accuracy = 0.43784672\n",
      "I0523 13:42:22.117991 139802773264256 run_pretraining.py:560]   masked_lm_accuracy = 0.43784672\n",
      "INFO:tensorflow:  masked_lm_loss = 3.1814866\n",
      "I0523 13:42:22.118090 139802773264256 run_pretraining.py:560]   masked_lm_loss = 3.1814866\n",
      "INFO:tensorflow:  sentence_order_accuracy = 0.7240625\n",
      "I0523 13:42:22.118172 139802773264256 run_pretraining.py:560]   sentence_order_accuracy = 0.7240625\n",
      "INFO:tensorflow:  sentence_order_loss = 0.49681297\n",
      "I0523 13:42:22.118247 139802773264256 run_pretraining.py:560]   sentence_order_loss = 0.49681297\n"
     ]
    }
   ],
   "source": [
    "!python \"./run_pretraining.py\" \\\n",
    "    --input_file \"./tf_examples\" \\\n",
    "    --output_dir \"./model\" \\\n",
    "    --init_checkpoint \"./bert_model.ckpt\" \\\n",
    "    --albert_config_file \"./albert_config.json\" \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --train_batch_size=64 \\\n",
    "    --eval_batch_size=32 \\\n",
    "    --max_seq_length 50 \\\n",
    "    --max_predictions_per_seq 8 \\\n",
    "    --optimizer 'lamb' \\\n",
    "    --learning_rate .00176 \\\n",
    "    --num_train_steps 100000 \\\n",
    "    --num_warmup_steps 1000 \\\n",
    "    --save_checkpoints_steps 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting Tensorflow Checkpoint file to Pytorch dump\n",
    "To use the HuggingFace library to realize our trained ALBERT model we need to convert our tensorflow checkpoint file to a pytorch dump."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Zu7JFNY1TIfv",
    "outputId": "bca4984e-0da8-45da-9138-c7c0cb35cdbf",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building PyTorch model from configuration: AlbertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"classifier_dropout_prob\": 0.1,\n",
      "  \"down_scale_factor\": 1,\n",
      "  \"embedding_size\": 128,\n",
      "  \"eos_token_id\": 3,\n",
      "  \"gap_size\": 0,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"inner_group_num\": 1,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"albert\",\n",
      "  \"net_structure_type\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_groups\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_memory_blocks\": 0,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.modeling_albert:Converting TensorFlow checkpoint from /content/drive/My Drive/BERT Training/model/model.ckpt-100000\n",
      "INFO:transformers.modeling_albert:Loading TF weight bert/embeddings/LayerNorm/beta with shape [128]\n",
      "INFO:transformers.modeling_albert:Loading TF weight bert/embeddings/LayerNorm/beta/adam_m with shape [128]\n",
      "INFO:transformers.modeling_albert:Loading TF weight bert/embeddings/LayerNorm/beta/adam_v with shape [128]\n",
      "INFO:transformers.modeling_albert:Loading TF weight bert/embeddings/LayerNorm/gamma with shape [128]\n",
      "INFO:transformers.modeling_albert:Loading TF weight bert/embeddings/LayerNorm/gamma/adam_m with shape [128]\n",
      "INFO:transformers.modeling_albert:Loading TF weight bert/embeddings/LayerNorm/gamma/adam_v with shape [128]\n",
      "INFO:transformers.modeling_albert:Loading TF weight bert/embeddings/position_embeddings with shape [512, 128]\n",
      "INFO:transformers.modeling_albert:Loading TF weight bert/embeddings/position_embeddings/adam_m with shape [512, 128]\n",
      "INFO:transformers.modeling_albert:Loading TF weight bert/embeddings/position_embeddings/adam_v with shape [512, 128]\n",
      "INFO:transformers.modeling_albert:Loading TF weight bert/embeddings/token_type_embeddings with shape [2, 128]\n",
      "INFO:transformers.modeling_albert:Loading TF weight bert/embeddings/token_type_embeddings/adam_m with shape [2, 128]\n",
      "INFO:transformers.modeling_albert:Loading TF weight bert/embeddings/token_type_embeddings/adam_v with shape [2, 128]\n",
      "INFO:transformers.modeling_albert:Loading TF weight bert/embeddings/word_embeddings with shape [30000, 128]\n",
      "INFO:transformers.modeling_albert:Loading TF weight bert/embeddings/word_embeddings/adam_m with shape [30000, 128]\n",
      "INFO:transformers.modeling_albert:Loading TF weight bert/embeddings/word_embeddings/adam_v with shape [30000, 128]\n",
      "INFO:transformers.modeling_albert:Loading TF weight bert/encoder/embedding_hidden_mapping_in/bias with shape [768]\n",
      "INFO:transformers.modeling_albert:Loading TF weight bert/encoder/embedding_hidden_mapping_in/bias/adam_m with shape [768]\n",
      "INFO:transformers.modeling_albert:Loading TF weight bert/encoder/embedding_hidden_mapping_in/bias/adam_v with shape [768]\n",
      "INFO:transformers.modeling_albert:Loading TF weight bert/encoder/embedding_hidden_mapping_in/kernel with shape [128, 768]\n",
      "INFO:transformers.modeling_albert:Loading TF weight bert/encoder/embedding_hidden_mapping_in/kernel/adam_m with shape [128, 768]\n",
      "INFO:transformers.modeling_albert:Loading TF weight bert/encoder/embedding_hidden_mapping_in/kernel/adam_v with shape [128, 768]\n",
      "INFO:transformers.modeling_albert:Loading TF weight bert/encoder/transformer/group_0/inner_group_0/LayerNorm/beta with shape [768]\n",
      "INFO:transformers.modeling_albert:Loading TF weight bert/encoder/transformer/group_0/inner_group_0/LayerNorm/beta/adam_m with shape [768]\n",
      "INFO:transformers.modeling_albert:Loading TF weight bert/encoder/transformer/group_0/inner_group_0/LayerNorm/beta/adam_v with shape [768]\n",
      "INFO:transformers.modeling_albert:Loading TF weight bert/encoder/transformer/group_0/inner_group_0/LayerNorm/gamma with shape [768]\n",
      "INFO:transformers.modeling_albert:Loading TF weight bert/encoder/transformer/group_0/inner_group_0/LayerNorm/gamma/adam_m with shape [768]\n",
      "INFO:transformers.modeling_albert:Loading TF weight bert/encoder/transformer/group_0/inner_group_0/LayerNorm/gamma/adam_v with shape [768]\n",
      "INFO:transformers.modeling_albert:Loading TF weight bert/encoder/transformer/group_0/inner_group_0/LayerNorm_1/beta with shape [768]\n",
      "INFO:transformers.modeling_albert:Loading TF weight bert/encoder/transformer/group_0/inner_group_0/LayerNorm_1/beta/adam_m with shape [768]\n",
      "INFO:transformers.modeling_albert:Loading TF weight bert/encoder/transformer/group_0/inner_group_0/LayerNorm_1/beta/adam_v with shape [768]\n",
      "INFO:transformers.modeling_albert:Loading TF weight bert/encoder/transformer/group_0/inner_group_0/LayerNorm_1/gamma with shape [768]\n",
      "INFO:transformers.modeling_albert:Loading TF weight bert/encoder/transformer/group_0/inner_group_0/LayerNorm_1/gamma/adam_m with shape [768]\n",
      "INFO:transformers.modeling_albert:Loading TF weight bert/encoder/transformer/group_0/inner_group_0/LayerNorm_1/gamma/adam_v with shape [768]\n",
      "INFO:transformers.modeling_albert:Loading TF weight bert/encoder/transformer/group_0/inner_group_0/attention_1/output/dense/bias with shape [768]\n",
      "INFO:transformers.modeling_albert:Loading TF weight bert/encoder/transformer/group_0/inner_group_0/attention_1/output/dense/bias/adam_m with shape [768]\n",
      "INFO:transformers.modeling_albert:Loading TF weight bert/encoder/transformer/group_0/inner_group_0/attention_1/output/dense/bias/adam_v with shape [768]\n",
      "INFO:transformers.modeling_albert:Loading TF weight bert/encoder/transformer/group_0/inner_group_0/attention_1/output/dense/kernel with shape [768, 768]\n",
      "INFO:transformers.modeling_albert:Loading TF weight bert/encoder/transformer/group_0/inner_group_0/attention_1/output/dense/kernel/adam_m with shape [768, 768]\n",
      "INFO:transformers.modeling_albert:Loading TF weight bert/encoder/transformer/group_0/inner_group_0/attention_1/output/dense/kernel/adam_v with shape [768, 768]\n",
      "INFO:transformers.modeling_albert:Loading TF weight bert/encoder/transformer/group_0/inner_group_0/attention_1/self/key/bias with shape [768]\n",
      "INFO:transformers.modeling_albert:Loading TF weight bert/encoder/transformer/group_0/inner_group_0/attention_1/self/key/bias/adam_m with shape [768]\n",
      "INFO:transformers.modeling_albert:Loading TF weight bert/encoder/transformer/group_0/inner_group_0/attention_1/self/key/bias/adam_v with shape [768]\n",
      "INFO:transformers.modeling_albert:Loading TF weight bert/encoder/transformer/group_0/inner_group_0/attention_1/self/key/kernel with shape [768, 768]\n",
      "INFO:transformers.modeling_albert:Loading TF weight bert/encoder/transformer/group_0/inner_group_0/attention_1/self/key/kernel/adam_m with shape [768, 768]\n",
      "INFO:transformers.modeling_albert:Loading TF weight bert/encoder/transformer/group_0/inner_group_0/attention_1/self/key/kernel/adam_v with shape [768, 768]\n",
      "INFO:transformers.modeling_albert:Loading TF weight bert/encoder/transformer/group_0/inner_group_0/attention_1/self/query/bias with shape [768]\n",
      "INFO:transformers.modeling_albert:Loading TF weight bert/encoder/transformer/group_0/inner_group_0/attention_1/self/query/bias/adam_m with shape [768]\n",
      "INFO:transformers.modeling_albert:Loading TF weight bert/encoder/transformer/group_0/inner_group_0/attention_1/self/query/bias/adam_v with shape [768]\n",
      "INFO:transformers.modeling_albert:Loading TF weight bert/encoder/transformer/group_0/inner_group_0/attention_1/self/query/kernel with shape [768, 768]\n",
      "INFO:transformers.modeling_albert:Loading TF weight bert/encoder/transformer/group_0/inner_group_0/attention_1/self/query/kernel/adam_m with shape [768, 768]\n",
      "INFO:transformers.modeling_albert:Loading TF weight bert/encoder/transformer/group_0/inner_group_0/attention_1/self/query/kernel/adam_v with shape [768, 768]\n",
      "INFO:transformers.modeling_albert:Loading TF weight bert/encoder/transformer/group_0/inner_group_0/attention_1/self/value/bias with shape [768]\n",
      "INFO:transformers.modeling_albert:Loading TF weight bert/encoder/transformer/group_0/inner_group_0/attention_1/self/value/bias/adam_m with shape [768]\n",
      "INFO:transformers.modeling_albert:Loading TF weight bert/encoder/transformer/group_0/inner_group_0/attention_1/self/value/bias/adam_v with shape [768]\n",
      "INFO:transformers.modeling_albert:Loading TF weight bert/encoder/transformer/group_0/inner_group_0/attention_1/self/value/kernel with shape [768, 768]\n",
      "INFO:transformers.modeling_albert:Loading TF weight bert/encoder/transformer/group_0/inner_group_0/attention_1/self/value/kernel/adam_m with shape [768, 768]\n",
      "INFO:transformers.modeling_albert:Loading TF weight bert/encoder/transformer/group_0/inner_group_0/attention_1/self/value/kernel/adam_v with shape [768, 768]\n",
      "INFO:transformers.modeling_albert:Loading TF weight bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/dense/bias with shape [3072]\n",
      "INFO:transformers.modeling_albert:Loading TF weight bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/dense/bias/adam_m with shape [3072]\n",
      "INFO:transformers.modeling_albert:Loading TF weight bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/dense/bias/adam_v with shape [3072]\n",
      "INFO:transformers.modeling_albert:Loading TF weight bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/dense/kernel with shape [768, 3072]\n",
      "INFO:transformers.modeling_albert:Loading TF weight bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/dense/kernel/adam_m with shape [768, 3072]\n",
      "INFO:transformers.modeling_albert:Loading TF weight bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/dense/kernel/adam_v with shape [768, 3072]\n",
      "INFO:transformers.modeling_albert:Loading TF weight bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/output/dense/bias with shape [768]\n",
      "INFO:transformers.modeling_albert:Loading TF weight bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/output/dense/bias/adam_m with shape [768]\n",
      "INFO:transformers.modeling_albert:Loading TF weight bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/output/dense/bias/adam_v with shape [768]\n",
      "INFO:transformers.modeling_albert:Loading TF weight bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/output/dense/kernel with shape [3072, 768]\n",
      "INFO:transformers.modeling_albert:Loading TF weight bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/output/dense/kernel/adam_m with shape [3072, 768]\n",
      "INFO:transformers.modeling_albert:Loading TF weight bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/output/dense/kernel/adam_v with shape [3072, 768]\n",
      "INFO:transformers.modeling_albert:Loading TF weight bert/pooler/dense/bias with shape [768]\n",
      "INFO:transformers.modeling_albert:Loading TF weight bert/pooler/dense/bias/adam_m with shape [768]\n",
      "INFO:transformers.modeling_albert:Loading TF weight bert/pooler/dense/bias/adam_v with shape [768]\n",
      "INFO:transformers.modeling_albert:Loading TF weight bert/pooler/dense/kernel with shape [768, 768]\n",
      "INFO:transformers.modeling_albert:Loading TF weight bert/pooler/dense/kernel/adam_m with shape [768, 768]\n",
      "INFO:transformers.modeling_albert:Loading TF weight bert/pooler/dense/kernel/adam_v with shape [768, 768]\n",
      "INFO:transformers.modeling_albert:Loading TF weight cls/predictions/output_bias with shape [30000]\n",
      "INFO:transformers.modeling_albert:Loading TF weight cls/predictions/output_bias/adam_m with shape [30000]\n",
      "INFO:transformers.modeling_albert:Loading TF weight cls/predictions/output_bias/adam_v with shape [30000]\n",
      "INFO:transformers.modeling_albert:Loading TF weight cls/predictions/transform/LayerNorm/beta with shape [128]\n",
      "INFO:transformers.modeling_albert:Loading TF weight cls/predictions/transform/LayerNorm/beta/adam_m with shape [128]\n",
      "INFO:transformers.modeling_albert:Loading TF weight cls/predictions/transform/LayerNorm/beta/adam_v with shape [128]\n",
      "INFO:transformers.modeling_albert:Loading TF weight cls/predictions/transform/LayerNorm/gamma with shape [128]\n",
      "INFO:transformers.modeling_albert:Loading TF weight cls/predictions/transform/LayerNorm/gamma/adam_m with shape [128]\n",
      "INFO:transformers.modeling_albert:Loading TF weight cls/predictions/transform/LayerNorm/gamma/adam_v with shape [128]\n",
      "INFO:transformers.modeling_albert:Loading TF weight cls/predictions/transform/dense/bias with shape [128]\n",
      "INFO:transformers.modeling_albert:Loading TF weight cls/predictions/transform/dense/bias/adam_m with shape [128]\n",
      "INFO:transformers.modeling_albert:Loading TF weight cls/predictions/transform/dense/bias/adam_v with shape [128]\n",
      "INFO:transformers.modeling_albert:Loading TF weight cls/predictions/transform/dense/kernel with shape [768, 128]\n",
      "INFO:transformers.modeling_albert:Loading TF weight cls/predictions/transform/dense/kernel/adam_m with shape [768, 128]\n",
      "INFO:transformers.modeling_albert:Loading TF weight cls/predictions/transform/dense/kernel/adam_v with shape [768, 128]\n",
      "INFO:transformers.modeling_albert:Loading TF weight cls/seq_relationship/output_bias with shape [2]\n",
      "INFO:transformers.modeling_albert:Loading TF weight cls/seq_relationship/output_bias/adam_m with shape [2]\n",
      "INFO:transformers.modeling_albert:Loading TF weight cls/seq_relationship/output_bias/adam_v with shape [2]\n",
      "INFO:transformers.modeling_albert:Loading TF weight cls/seq_relationship/output_weights with shape [2, 768]\n",
      "INFO:transformers.modeling_albert:Loading TF weight cls/seq_relationship/output_weights/adam_m with shape [2, 768]\n",
      "INFO:transformers.modeling_albert:Loading TF weight cls/seq_relationship/output_weights/adam_v with shape [2, 768]\n",
      "INFO:transformers.modeling_albert:Loading TF weight global_step with shape []\n",
      "INFO:transformers.modeling_albert:Skipping albert/embeddings/LayerNorm/beta/adam_m\n",
      "INFO:transformers.modeling_albert:Skipping albert/embeddings/LayerNorm/beta/adam_v\n",
      "INFO:transformers.modeling_albert:Skipping albert/embeddings/LayerNorm/gamma/adam_m\n",
      "INFO:transformers.modeling_albert:Skipping albert/embeddings/LayerNorm/gamma/adam_v\n",
      "INFO:transformers.modeling_albert:Skipping albert/embeddings/position_embeddings/adam_m\n",
      "INFO:transformers.modeling_albert:Skipping albert/embeddings/position_embeddings/adam_v\n",
      "INFO:transformers.modeling_albert:Skipping albert/embeddings/token_type_embeddings/adam_m\n",
      "INFO:transformers.modeling_albert:Skipping albert/embeddings/token_type_embeddings/adam_v\n",
      "INFO:transformers.modeling_albert:Skipping albert/embeddings/word_embeddings/adam_m\n",
      "INFO:transformers.modeling_albert:Skipping albert/embeddings/word_embeddings/adam_v\n",
      "INFO:transformers.modeling_albert:Skipping albert/encoder/embedding_hidden_mapping_in/bias/adam_m\n",
      "INFO:transformers.modeling_albert:Skipping albert/encoder/embedding_hidden_mapping_in/bias/adam_v\n",
      "INFO:transformers.modeling_albert:Skipping albert/encoder/embedding_hidden_mapping_in/kernel/adam_m\n",
      "INFO:transformers.modeling_albert:Skipping albert/encoder/embedding_hidden_mapping_in/kernel/adam_v\n",
      "INFO:transformers.modeling_albert:Skipping albert/encoder/albert_layer_groups/0/albert_layers/0/attention/LayerNorm/beta/adam_m\n",
      "INFO:transformers.modeling_albert:Skipping albert/encoder/albert_layer_groups/0/albert_layers/0/attention/LayerNorm/beta/adam_v\n",
      "INFO:transformers.modeling_albert:Skipping albert/encoder/albert_layer_groups/0/albert_layers/0/attention/LayerNorm/gamma/adam_m\n",
      "INFO:transformers.modeling_albert:Skipping albert/encoder/albert_layer_groups/0/albert_layers/0/attention/LayerNorm/gamma/adam_v\n",
      "INFO:transformers.modeling_albert:Skipping albert/encoder/albert_layer_groups/0/albert_layers/0/full_layer_layer_norm/beta/adam_m\n",
      "INFO:transformers.modeling_albert:Skipping albert/encoder/albert_layer_groups/0/albert_layers/0/full_layer_layer_norm/beta/adam_v\n",
      "INFO:transformers.modeling_albert:Skipping albert/encoder/albert_layer_groups/0/albert_layers/0/full_layer_layer_norm/gamma/adam_m\n",
      "INFO:transformers.modeling_albert:Skipping albert/encoder/albert_layer_groups/0/albert_layers/0/full_layer_layer_norm/gamma/adam_v\n",
      "INFO:transformers.modeling_albert:Skipping albert/encoder/albert_layer_groups/0/albert_layers/0/attention/dense/bias/adam_m\n",
      "INFO:transformers.modeling_albert:Skipping albert/encoder/albert_layer_groups/0/albert_layers/0/attention/dense/bias/adam_v\n",
      "INFO:transformers.modeling_albert:Skipping albert/encoder/albert_layer_groups/0/albert_layers/0/attention/dense/kernel/adam_m\n",
      "INFO:transformers.modeling_albert:Skipping albert/encoder/albert_layer_groups/0/albert_layers/0/attention/dense/kernel/adam_v\n",
      "INFO:transformers.modeling_albert:Skipping albert/encoder/albert_layer_groups/0/albert_layers/0/attention/key/bias/adam_m\n",
      "INFO:transformers.modeling_albert:Skipping albert/encoder/albert_layer_groups/0/albert_layers/0/attention/key/bias/adam_v\n",
      "INFO:transformers.modeling_albert:Skipping albert/encoder/albert_layer_groups/0/albert_layers/0/attention/key/kernel/adam_m\n",
      "INFO:transformers.modeling_albert:Skipping albert/encoder/albert_layer_groups/0/albert_layers/0/attention/key/kernel/adam_v\n",
      "INFO:transformers.modeling_albert:Skipping albert/encoder/albert_layer_groups/0/albert_layers/0/attention/query/bias/adam_m\n",
      "INFO:transformers.modeling_albert:Skipping albert/encoder/albert_layer_groups/0/albert_layers/0/attention/query/bias/adam_v\n",
      "INFO:transformers.modeling_albert:Skipping albert/encoder/albert_layer_groups/0/albert_layers/0/attention/query/kernel/adam_m\n",
      "INFO:transformers.modeling_albert:Skipping albert/encoder/albert_layer_groups/0/albert_layers/0/attention/query/kernel/adam_v\n",
      "INFO:transformers.modeling_albert:Skipping albert/encoder/albert_layer_groups/0/albert_layers/0/attention/value/bias/adam_m\n",
      "INFO:transformers.modeling_albert:Skipping albert/encoder/albert_layer_groups/0/albert_layers/0/attention/value/bias/adam_v\n",
      "INFO:transformers.modeling_albert:Skipping albert/encoder/albert_layer_groups/0/albert_layers/0/attention/value/kernel/adam_m\n",
      "INFO:transformers.modeling_albert:Skipping albert/encoder/albert_layer_groups/0/albert_layers/0/attention/value/kernel/adam_v\n",
      "INFO:transformers.modeling_albert:Skipping albert/encoder/albert_layer_groups/0/albert_layers/0/ffn/bias/adam_m\n",
      "INFO:transformers.modeling_albert:Skipping albert/encoder/albert_layer_groups/0/albert_layers/0/ffn/bias/adam_v\n",
      "INFO:transformers.modeling_albert:Skipping albert/encoder/albert_layer_groups/0/albert_layers/0/ffn/kernel/adam_m\n",
      "INFO:transformers.modeling_albert:Skipping albert/encoder/albert_layer_groups/0/albert_layers/0/ffn/kernel/adam_v\n",
      "INFO:transformers.modeling_albert:Skipping albert/encoder/albert_layer_groups/0/albert_layers/0/ffn_output/bias/adam_m\n",
      "INFO:transformers.modeling_albert:Skipping albert/encoder/albert_layer_groups/0/albert_layers/0/ffn_output/bias/adam_v\n",
      "INFO:transformers.modeling_albert:Skipping albert/encoder/albert_layer_groups/0/albert_layers/0/ffn_output/kernel/adam_m\n",
      "INFO:transformers.modeling_albert:Skipping albert/encoder/albert_layer_groups/0/albert_layers/0/ffn_output/kernel/adam_v\n",
      "INFO:transformers.modeling_albert:Skipping albert/pooler/bias/adam_m\n",
      "INFO:transformers.modeling_albert:Skipping albert/pooler/bias/adam_v\n",
      "INFO:transformers.modeling_albert:Skipping albert/pooler/kernel/adam_m\n",
      "INFO:transformers.modeling_albert:Skipping albert/pooler/kernel/adam_v\n",
      "INFO:transformers.modeling_albert:Skipping predictions/output_bias/adam_m\n",
      "INFO:transformers.modeling_albert:Skipping predictions/output_bias/adam_v\n",
      "INFO:transformers.modeling_albert:Skipping predictions/LayerNorm/beta/adam_m\n",
      "INFO:transformers.modeling_albert:Skipping predictions/LayerNorm/beta/adam_v\n",
      "INFO:transformers.modeling_albert:Skipping predictions/LayerNorm/gamma/adam_m\n",
      "INFO:transformers.modeling_albert:Skipping predictions/LayerNorm/gamma/adam_v\n",
      "INFO:transformers.modeling_albert:Skipping predictions/dense/bias/adam_m\n",
      "INFO:transformers.modeling_albert:Skipping predictions/dense/bias/adam_v\n",
      "INFO:transformers.modeling_albert:Skipping predictions/dense/kernel/adam_m\n",
      "INFO:transformers.modeling_albert:Skipping predictions/dense/kernel/adam_v\n",
      "INFO:transformers.modeling_albert:Skipping cls/sop_classifier/classifier/bias\n",
      "INFO:transformers.modeling_albert:Skipping cls/sop_classifier/classifier/bias/adam_m\n",
      "INFO:transformers.modeling_albert:Skipping cls/sop_classifier/classifier/bias/adam_v\n",
      "INFO:transformers.modeling_albert:Skipping cls/sop_classifier/classifier/weight\n",
      "INFO:transformers.modeling_albert:Skipping cls/sop_classifier/classifier/weight/adam_m\n",
      "INFO:transformers.modeling_albert:Skipping cls/sop_classifier/classifier/weight/adam_v\n",
      "INFO:transformers.modeling_albert:Skipping global_step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert/embeddings/LayerNorm/beta\n",
      "bert/embeddings/LayerNorm/beta/adam_m\n",
      "bert/embeddings/LayerNorm/beta/adam_v\n",
      "bert/embeddings/LayerNorm/gamma\n",
      "bert/embeddings/LayerNorm/gamma/adam_m\n",
      "bert/embeddings/LayerNorm/gamma/adam_v\n",
      "bert/embeddings/position_embeddings\n",
      "bert/embeddings/position_embeddings/adam_m\n",
      "bert/embeddings/position_embeddings/adam_v\n",
      "bert/embeddings/token_type_embeddings\n",
      "bert/embeddings/token_type_embeddings/adam_m\n",
      "bert/embeddings/token_type_embeddings/adam_v\n",
      "bert/embeddings/word_embeddings\n",
      "bert/embeddings/word_embeddings/adam_m\n",
      "bert/embeddings/word_embeddings/adam_v\n",
      "bert/encoder/embedding_hidden_mapping_in/bias\n",
      "bert/encoder/embedding_hidden_mapping_in/bias/adam_m\n",
      "bert/encoder/embedding_hidden_mapping_in/bias/adam_v\n",
      "bert/encoder/embedding_hidden_mapping_in/kernel\n",
      "bert/encoder/embedding_hidden_mapping_in/kernel/adam_m\n",
      "bert/encoder/embedding_hidden_mapping_in/kernel/adam_v\n",
      "bert/encoder/transformer/group_0/inner_group_0/LayerNorm/beta\n",
      "bert/encoder/transformer/group_0/inner_group_0/LayerNorm/beta/adam_m\n",
      "bert/encoder/transformer/group_0/inner_group_0/LayerNorm/beta/adam_v\n",
      "bert/encoder/transformer/group_0/inner_group_0/LayerNorm/gamma\n",
      "bert/encoder/transformer/group_0/inner_group_0/LayerNorm/gamma/adam_m\n",
      "bert/encoder/transformer/group_0/inner_group_0/LayerNorm/gamma/adam_v\n",
      "bert/encoder/transformer/group_0/inner_group_0/LayerNorm_1/beta\n",
      "bert/encoder/transformer/group_0/inner_group_0/LayerNorm_1/beta/adam_m\n",
      "bert/encoder/transformer/group_0/inner_group_0/LayerNorm_1/beta/adam_v\n",
      "bert/encoder/transformer/group_0/inner_group_0/LayerNorm_1/gamma\n",
      "bert/encoder/transformer/group_0/inner_group_0/LayerNorm_1/gamma/adam_m\n",
      "bert/encoder/transformer/group_0/inner_group_0/LayerNorm_1/gamma/adam_v\n",
      "bert/encoder/transformer/group_0/inner_group_0/attention_1/output/dense/bias\n",
      "bert/encoder/transformer/group_0/inner_group_0/attention_1/output/dense/bias/adam_m\n",
      "bert/encoder/transformer/group_0/inner_group_0/attention_1/output/dense/bias/adam_v\n",
      "bert/encoder/transformer/group_0/inner_group_0/attention_1/output/dense/kernel\n",
      "bert/encoder/transformer/group_0/inner_group_0/attention_1/output/dense/kernel/adam_m\n",
      "bert/encoder/transformer/group_0/inner_group_0/attention_1/output/dense/kernel/adam_v\n",
      "bert/encoder/transformer/group_0/inner_group_0/attention_1/self/key/bias\n",
      "bert/encoder/transformer/group_0/inner_group_0/attention_1/self/key/bias/adam_m\n",
      "bert/encoder/transformer/group_0/inner_group_0/attention_1/self/key/bias/adam_v\n",
      "bert/encoder/transformer/group_0/inner_group_0/attention_1/self/key/kernel\n",
      "bert/encoder/transformer/group_0/inner_group_0/attention_1/self/key/kernel/adam_m\n",
      "bert/encoder/transformer/group_0/inner_group_0/attention_1/self/key/kernel/adam_v\n",
      "bert/encoder/transformer/group_0/inner_group_0/attention_1/self/query/bias\n",
      "bert/encoder/transformer/group_0/inner_group_0/attention_1/self/query/bias/adam_m\n",
      "bert/encoder/transformer/group_0/inner_group_0/attention_1/self/query/bias/adam_v\n",
      "bert/encoder/transformer/group_0/inner_group_0/attention_1/self/query/kernel\n",
      "bert/encoder/transformer/group_0/inner_group_0/attention_1/self/query/kernel/adam_m\n",
      "bert/encoder/transformer/group_0/inner_group_0/attention_1/self/query/kernel/adam_v\n",
      "bert/encoder/transformer/group_0/inner_group_0/attention_1/self/value/bias\n",
      "bert/encoder/transformer/group_0/inner_group_0/attention_1/self/value/bias/adam_m\n",
      "bert/encoder/transformer/group_0/inner_group_0/attention_1/self/value/bias/adam_v\n",
      "bert/encoder/transformer/group_0/inner_group_0/attention_1/self/value/kernel\n",
      "bert/encoder/transformer/group_0/inner_group_0/attention_1/self/value/kernel/adam_m\n",
      "bert/encoder/transformer/group_0/inner_group_0/attention_1/self/value/kernel/adam_v\n",
      "bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/dense/bias\n",
      "bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/dense/bias/adam_m\n",
      "bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/dense/bias/adam_v\n",
      "bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/dense/kernel\n",
      "bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/dense/kernel/adam_m\n",
      "bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/dense/kernel/adam_v\n",
      "bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/output/dense/bias\n",
      "bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/output/dense/bias/adam_m\n",
      "bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/output/dense/bias/adam_v\n",
      "bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/output/dense/kernel\n",
      "bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/output/dense/kernel/adam_m\n",
      "bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/output/dense/kernel/adam_v\n",
      "bert/pooler/dense/bias\n",
      "bert/pooler/dense/bias/adam_m\n",
      "bert/pooler/dense/bias/adam_v\n",
      "bert/pooler/dense/kernel\n",
      "bert/pooler/dense/kernel/adam_m\n",
      "bert/pooler/dense/kernel/adam_v\n",
      "cls/predictions/output_bias\n",
      "cls/predictions/output_bias/adam_m\n",
      "cls/predictions/output_bias/adam_v\n",
      "cls/predictions/transform/LayerNorm/beta\n",
      "cls/predictions/transform/LayerNorm/beta/adam_m\n",
      "cls/predictions/transform/LayerNorm/beta/adam_v\n",
      "cls/predictions/transform/LayerNorm/gamma\n",
      "cls/predictions/transform/LayerNorm/gamma/adam_m\n",
      "cls/predictions/transform/LayerNorm/gamma/adam_v\n",
      "cls/predictions/transform/dense/bias\n",
      "cls/predictions/transform/dense/bias/adam_m\n",
      "cls/predictions/transform/dense/bias/adam_v\n",
      "cls/predictions/transform/dense/kernel\n",
      "cls/predictions/transform/dense/kernel/adam_m\n",
      "cls/predictions/transform/dense/kernel/adam_v\n",
      "cls/seq_relationship/output_bias\n",
      "cls/seq_relationship/output_bias/adam_m\n",
      "cls/seq_relationship/output_bias/adam_v\n",
      "cls/seq_relationship/output_weights\n",
      "cls/seq_relationship/output_weights/adam_m\n",
      "cls/seq_relationship/output_weights/adam_v\n",
      "global_step\n",
      "Initialize PyTorch weight ['albert', 'embeddings', 'LayerNorm', 'beta'] from bert/embeddings/LayerNorm/beta\n",
      "Initialize PyTorch weight ['albert', 'embeddings', 'LayerNorm', 'gamma'] from bert/embeddings/LayerNorm/gamma\n",
      "Initialize PyTorch weight ['albert', 'embeddings', 'position_embeddings'] from bert/embeddings/position_embeddings\n",
      "Initialize PyTorch weight ['albert', 'embeddings', 'token_type_embeddings'] from bert/embeddings/token_type_embeddings\n",
      "Initialize PyTorch weight ['albert', 'embeddings', 'word_embeddings'] from bert/embeddings/word_embeddings\n",
      "Initialize PyTorch weight ['albert', 'encoder', 'embedding_hidden_mapping_in', 'bias'] from bert/encoder/embedding_hidden_mapping_in/bias\n",
      "Initialize PyTorch weight ['albert', 'encoder', 'embedding_hidden_mapping_in', 'kernel'] from bert/encoder/embedding_hidden_mapping_in/kernel\n",
      "Initialize PyTorch weight ['albert', 'encoder', 'albert_layer_groups', '0', 'albert_layers', '0', 'attention', 'LayerNorm', 'beta'] from bert/encoder/transformer/group_0/inner_group_0/LayerNorm/beta\n",
      "Initialize PyTorch weight ['albert', 'encoder', 'albert_layer_groups', '0', 'albert_layers', '0', 'attention', 'LayerNorm', 'gamma'] from bert/encoder/transformer/group_0/inner_group_0/LayerNorm/gamma\n",
      "Initialize PyTorch weight ['albert', 'encoder', 'albert_layer_groups', '0', 'albert_layers', '0', 'full_layer_layer_norm', 'beta'] from bert/encoder/transformer/group_0/inner_group_0/LayerNorm_1/beta\n",
      "Initialize PyTorch weight ['albert', 'encoder', 'albert_layer_groups', '0', 'albert_layers', '0', 'full_layer_layer_norm', 'gamma'] from bert/encoder/transformer/group_0/inner_group_0/LayerNorm_1/gamma\n",
      "Initialize PyTorch weight ['albert', 'encoder', 'albert_layer_groups', '0', 'albert_layers', '0', 'attention', 'dense', 'bias'] from bert/encoder/transformer/group_0/inner_group_0/attention_1/output/dense/bias\n",
      "Initialize PyTorch weight ['albert', 'encoder', 'albert_layer_groups', '0', 'albert_layers', '0', 'attention', 'dense', 'kernel'] from bert/encoder/transformer/group_0/inner_group_0/attention_1/output/dense/kernel\n",
      "Initialize PyTorch weight ['albert', 'encoder', 'albert_layer_groups', '0', 'albert_layers', '0', 'attention', 'key', 'bias'] from bert/encoder/transformer/group_0/inner_group_0/attention_1/self/key/bias\n",
      "Initialize PyTorch weight ['albert', 'encoder', 'albert_layer_groups', '0', 'albert_layers', '0', 'attention', 'key', 'kernel'] from bert/encoder/transformer/group_0/inner_group_0/attention_1/self/key/kernel\n",
      "Initialize PyTorch weight ['albert', 'encoder', 'albert_layer_groups', '0', 'albert_layers', '0', 'attention', 'query', 'bias'] from bert/encoder/transformer/group_0/inner_group_0/attention_1/self/query/bias\n",
      "Initialize PyTorch weight ['albert', 'encoder', 'albert_layer_groups', '0', 'albert_layers', '0', 'attention', 'query', 'kernel'] from bert/encoder/transformer/group_0/inner_group_0/attention_1/self/query/kernel\n",
      "Initialize PyTorch weight ['albert', 'encoder', 'albert_layer_groups', '0', 'albert_layers', '0', 'attention', 'value', 'bias'] from bert/encoder/transformer/group_0/inner_group_0/attention_1/self/value/bias\n",
      "Initialize PyTorch weight ['albert', 'encoder', 'albert_layer_groups', '0', 'albert_layers', '0', 'attention', 'value', 'kernel'] from bert/encoder/transformer/group_0/inner_group_0/attention_1/self/value/kernel\n",
      "Initialize PyTorch weight ['albert', 'encoder', 'albert_layer_groups', '0', 'albert_layers', '0', 'ffn', 'bias'] from bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/dense/bias\n",
      "Initialize PyTorch weight ['albert', 'encoder', 'albert_layer_groups', '0', 'albert_layers', '0', 'ffn', 'kernel'] from bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/dense/kernel\n",
      "Initialize PyTorch weight ['albert', 'encoder', 'albert_layer_groups', '0', 'albert_layers', '0', 'ffn_output', 'bias'] from bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/output/dense/bias\n",
      "Initialize PyTorch weight ['albert', 'encoder', 'albert_layer_groups', '0', 'albert_layers', '0', 'ffn_output', 'kernel'] from bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/output/dense/kernel\n",
      "Initialize PyTorch weight ['albert', 'pooler', 'bias'] from bert/pooler/dense/bias\n",
      "Initialize PyTorch weight ['albert', 'pooler', 'kernel'] from bert/pooler/dense/kernel\n",
      "Initialize PyTorch weight ['predictions', 'output_bias'] from cls/predictions/output_bias\n",
      "Initialize PyTorch weight ['predictions', 'LayerNorm', 'beta'] from cls/predictions/transform/LayerNorm/beta\n",
      "Initialize PyTorch weight ['predictions', 'LayerNorm', 'gamma'] from cls/predictions/transform/LayerNorm/gamma\n",
      "Initialize PyTorch weight ['predictions', 'dense', 'bias'] from cls/predictions/transform/dense/bias\n",
      "Initialize PyTorch weight ['predictions', 'dense', 'kernel'] from cls/predictions/transform/dense/kernel\n",
      "Initialize PyTorch weight ['cls', 'sop_classifier', 'classifier', 'bias'] from cls/seq_relationship/output_bias\n",
      "Initialize PyTorch weight ['cls', 'sop_classifier', 'classifier', 'weight'] from cls/seq_relationship/output_weights\n",
      "Save PyTorch model to /content/drive/My Drive/BERT Training/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import torch\n",
    "from transformers import AlbertConfig, AlbertForPreTraining, load_tf_weights_in_albert\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "\n",
    "def convert_tf_checkpoint_to_pytorch(tf_checkpoint_path, albert_config_file, pytorch_dump_path):\n",
    "    # Initialise PyTorch model\n",
    "    config = AlbertConfig.from_json_file(albert_config_file)\n",
    "    print(\"Building PyTorch model from configuration: {}\".format(str(config)))\n",
    "    model = AlbertForPreTraining(config)\n",
    "\n",
    "    # Load weights from tf checkpoint\n",
    "    load_tf_weights_in_albert(model, config, tf_checkpoint_path)\n",
    "\n",
    "    # Save pytorch-model\n",
    "    print(\"Save PyTorch model to {}\".format(pytorch_dump_path))\n",
    "    torch.save(model.state_dict(), pytorch_dump_path)\n",
    "\n",
    "\n",
    "\n",
    "convert_tf_checkpoint_to_pytorch('model.ckpt-100000','albert_config.json' , 'pytorch_model.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the trained ALBERT model\n",
    "Now that we have our pytorch_model.bin file with few steps we can make a functional model to help us generate embeddings for our docstrings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the albert tokenizer from HuggingFace library\n",
    "albert_tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlbertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"classifier_dropout_prob\": 0.1,\n",
      "  \"down_scale_factor\": 1,\n",
      "  \"embedding_size\": 128,\n",
      "  \"eos_token_id\": 3,\n",
      "  \"gap_size\": 0,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"inner_group_num\": 1,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"albert\",\n",
      "  \"net_structure_type\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_groups\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_memory_blocks\": 0,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Loading the weights and the configuration of our trained ALBERT model\n",
    "config = AlbertConfig.from_pretrained('./albert', output_hidden_states=True)\n",
    "\n",
    "model = TFAlbertModel.from_pretrained('./albert', config=config,  from_pt=True)\n",
    "print(model.config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Embeddings for Docstrings\n",
    "Now that our ALBERT model is ready we can churn out some docstring vectors from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the train data\n",
    "train_df = pd.read_csv('train_sorted.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "996341\n"
     ]
    }
   ],
   "source": [
    "# Number of docstrings\n",
    "print(len(train_df['docstring'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_embeddings = []\n",
    "for count,item in enumerate(train_df['docstring'].values): #traverse thorugh all train data set docstrings\n",
    "    e = albert_tokenizer.encode(item, max_length=512)\n",
    "    input = tf.constant(e)[None, :]  # Batch size 1 \n",
    "    output = model(input)\n",
    "    v = [0]*768\n",
    "    for i in range(1, len(input[0])-1):\n",
    "        v = v + output[0][0][i].numpy()  # generate sentence vectors by averaging the word vectors\n",
    "    avg_embeddings.append(v/len(input[0])) #append all sentence vectors into a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the sentence embeddings in a .tsv file\n",
    "with open(\"avg_embeddings.tsv\",\"w+\",newline='') as my_csv:\n",
    "    csvWriter = csv.writer(my_csv,delimiter='\\t')\n",
    "    csvWriter.writerows(avg_embeddings)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ALBERT",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0b3ca68682754d908d3a8f317f808ea7": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0b8b16e529304a2ca648fe58050a0d25": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0fc85d5f3fbc44e987f34da75dd2dff3": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "11ab4df98a404a72a31b4e44040e98c7": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1d16f38782b74dfeb56dfddae052cb44": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_11ab4df98a404a72a31b4e44040e98c7",
      "max": 760289,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_793cd33e4b4a4aa3a5cf6fe98fcc26c2",
      "value": 760289
     }
    },
    "1e1f4cc5043f40529ea4d9b071534f7a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_210643d6ba4e4e518547f49b7c45310a",
      "placeholder": "​",
      "style": "IPY_MODEL_38fc59c3e0be41f589e185a103af7697",
      "value": " 760k/760k [01:50&lt;00:00, 6.87kB/s]"
     }
    },
    "210643d6ba4e4e518547f49b7c45310a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2b75e668c35b48418f1d4aca480e0ad8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0b3ca68682754d908d3a8f317f808ea7",
      "placeholder": "​",
      "style": "IPY_MODEL_c1c9dbf1617942ada2e83ddc9e001ca9",
      "value": " 760k/760k [00:21&lt;00:00, 35.1kB/s]"
     }
    },
    "38fc59c3e0be41f589e185a103af7697": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "793cd33e4b4a4aa3a5cf6fe98fcc26c2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "8214c57d8fdd46a5a5721fd39c8c54b1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cee8d498182e4faba7cdcb1c1123bc46",
      "max": 760289,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e41fddc01fc9458098115fb7a2cc38ee",
      "value": 760289
     }
    },
    "849d6f340e134c538a92b0bdf93472a3": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "85946b5ddb794fdea01de3d7d06c8a42": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "872fd3d0c028441fa086bd394e2d98f3": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "96e5abfa48a549c2ae801fecb01d4fcb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_872fd3d0c028441fa086bd394e2d98f3",
      "placeholder": "​",
      "style": "IPY_MODEL_849d6f340e134c538a92b0bdf93472a3",
      "value": " 760k/760k [18:46&lt;00:00, 675B/s]"
     }
    },
    "9cf892ee1908476aa258f6b21defb484": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8214c57d8fdd46a5a5721fd39c8c54b1",
       "IPY_MODEL_2b75e668c35b48418f1d4aca480e0ad8"
      ],
      "layout": "IPY_MODEL_0fc85d5f3fbc44e987f34da75dd2dff3"
     }
    },
    "9e3303deebb74dc8882fa0e75dfbb3e6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_cc94afe80d284581ab876ecada566cb1",
       "IPY_MODEL_96e5abfa48a549c2ae801fecb01d4fcb"
      ],
      "layout": "IPY_MODEL_dafe291aa99e422c9e75677ff7ee9a76"
     }
    },
    "c1c9dbf1617942ada2e83ddc9e001ca9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cc94afe80d284581ab876ecada566cb1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0b8b16e529304a2ca648fe58050a0d25",
      "max": 760289,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_df52eefbce284701b29a94260c414ccd",
      "value": 760289
     }
    },
    "cee8d498182e4faba7cdcb1c1123bc46": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dafe291aa99e422c9e75677ff7ee9a76": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dcdd29ba14b6414eb0a4a2c4dfd2dbec": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1d16f38782b74dfeb56dfddae052cb44",
       "IPY_MODEL_1e1f4cc5043f40529ea4d9b071534f7a"
      ],
      "layout": "IPY_MODEL_85946b5ddb794fdea01de3d7d06c8a42"
     }
    },
    "df52eefbce284701b29a94260c414ccd": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "e41fddc01fc9458098115fb7a2cc38ee": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
